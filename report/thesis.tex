\documentclass[twoside,11pt,openright]{report}

\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{a4}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{epsfig}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{datetime}
\usepackage{ifpdf}
\ifpdf
  \usepackage{epstopdf}
\fi
\usepackage{textcomp}
\usepackage{wasysym}
\usepackage{epigraph}
\usepackage{wrapfig}

% chapter headings
\usepackage{quotchap}
\makeatletter
\renewcommand*{\sectfont}{\bfseries}
\renewcommand*{\chapnumfont}{%
  \usefont{T1}{\@defaultcnfont}{b}{n}\fontsize{50}{70}\selectfont% Default: 100/130
  \color{chaptergrey}%
}
\makeatother

% This font looks so good.
\usepackage[sc]{mathpazo}

% Typesetting pseudo-code
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}

% better epsilon
\def \epsilon {\varepsilon}

% better bar
\def \bar {\widetilde}

% Inline list
\usepackage[inline]{enumitem}
\newlist{inlinelist}{enumerate*}{1}
\setlist[inlinelist]{label=(\roman*)}

% tikz
\usepackage{tikz}
% git graph
\newcommand\commit[3]{\node[commit] (#1) {}; \node[clabel] at (#1) {\texttt{#1}: (\textit{#2}) #3};}
\newcommand\ghost[1]{\coordinate (#1);}
\newcommand\connect[2]{\path (#1) to[out=90,in=-90] (#2);}

% Code comments like [CLRS]
\renewcommand{\algorithmiccomment}[1]{\makebox[5cm][l]{$\triangleright$ \textit{#1}}}
\usepackage{framed,graphicx,xcolor}
\usepackage[space]{grffile}
\usepackage[font={small,it}]{caption}
\usepackage{listings}
\usepackage{units}

% theorems
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

% Relative references
\usepackage{varioref}

\usepackage[hidelinks]{hyperref}
\usepackage{url}

% Set fonts for section
%\usepackage{titlesec}
%\titleformat*{\section}{\LARGE\bfseries}
%\titleformat*{\subsection}{\large\bfseries}

\bibliographystyle{alpha}

\renewcommand*\ttdefault{txtt}

\newcommand{\todo}[1]{{\color[rgb]{.5,0,0}\textbf{$\blacktriangleright$#1$\blacktriangleleft$}}}

\begin{document}

\pagestyle{empty} 
\vspace*{\fill}\noindent{\rule{\linewidth}{1mm}\\[4ex]
{\Huge\sf Three-sided Range Reporting in\\External Memory}\\[2ex]
{\huge\sf Peter Gabrielsen, 20114179}\\[2ex]
{\huge\sf Christoffer Holb{\ae}k Hansen, 20114637}\\[2ex]
\noindent\rule{\linewidth}{1mm}\\[4ex]
\noindent{\Large\sf Master's Thesis, Computer Science\\[1ex] 
\monthname\ \the\year  \\[1ex] Project Advisor: Kasper Green Larsen
\\[1ex] Formal Advisor: Gerth St{\o}lting Brodal\\[15ex]}\\[\fill]}
\epsfig{file=logo.eps}\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\pagenumbering{roman}
\chapter*{Abstract}
\pagestyle{plain}
\pagenumbering{roman}
\addcontentsline{toc}{chapter}{Abstract}

The world is becoming increasingly data driven and almost anything in our daily lives generate data. The generated data sets are typically larger than what we can store in a normal computer's main memory. We therefore need clever techniques to handle and analyse the large data sets in order to make sense of it all. Much of the data that is gathered is spatial in nature, i.e. contains e.g. GPS coordinates. Analysing coordinates require us to solve complicated problems within the field of computational geometry. One of such problems is the problem of three-sided range reporting. In this problem we are asked to maintain a dynamic set, $\mathcal{S}$, of $N$ points in $\mathbb{R}^2$. The set can be updated by inserting or deleting points. Any solution to the problem must be able to answer three-sided range queries, i.e. given a range of the type $\left[x_1,x_2\right]\times \left[y,\infty\right]$ we report points in $\mathcal{S} \cap \left[x_1,x_2\right]\times \left[y,\infty\right]$.

This thesis presents, implements, and experiments with several solutions to the problem of three-sided range reporting. The main focus of the thesis will be to show the power of data structures optimized for handling huge data sets by minimising the number of disk accesses.

We have implemented two external memory data structures that solves this problem: The External Memory Priority Search Tree by Arge et al.~\cite{arge_samoladas_vitter_1999} and the External Memory \textit{Buffered} Priority Search Tree by Brodal~\cite{DBLP:journals/corr/Brodal15}.

Besides comparing these two structures against each other we also compare them against our own implementation of the Priority Search Tree of McCreight~\cite{DBLP:journals/siamcomp/McCreight85}, and wrappers around MySQL, libspatialindex R*Tree, and Boost R-Tree.

Our results show that the External Memory Buffered Priority Search Tree outperforms all of the other structures as soon as the structures have to handle more data than the main memory capacity. MySQL without an index was significantly faster at inserting but as both the experiments with deletion and querying showed, the cost of not having an index on the table is too high. The Boost R-Tree proved to be the fastest of the internal memory data structures.

\todo{conclude more?}

\chapter*{Resum\'e}
\addcontentsline{toc}{chapter}{Resum\'e}

Flere og flere hverdagsapparater bliver koblet p{\aa} internettet. Dette har til form{\aa}l at personligg{\o}re og for at bedre kunne m{\aa}lrette produkter til forbrugeren. Det skaber samtidig k{\ae}mpe m{\ae}ngder af data --- st{\o}rre m{\ae}ngder end en almindelig computer kan klare i dens interne hukommelse. Vi har derfor brug for teknikker og redskaber til at finde hoved og hale i alt det genereret data. Meget af det genererede data indeholder information omkring vores position i form af GPS koordinater. For at h{\aa}ndtere GPS koordinater skal man typisk l{\o}se komplicerede problemer indenfor algoritmisk geometri. Et s{\aa}dan problem kunne eksempelvis v{\ae}re det tre-sidede interval rapporterings problem. Dette problem g{\aa}r ud p{\aa} at vedligeholde en dynamisk m{\ae}ngde, $\mathcal{S}$, indeholdende $N$ punkter i $\mathbb{R}^2$. M{\ae}ngden kan modificeres ved at inds{\ae}tte eller slette punkter. En l{\o}sning p{\aa} dette problem skal v{\ae}re i stand til at svare p{\aa} foresp{\o}rgsler af typen $[x_1,x_2] \times [y,\infty]$, dvs. rapportere alle punkter i $\mathcal{S} \cap [x_1,x_2] \times [y,\infty]$.

Vi vil i dette speciale pr{\ae}sentere, implementere og eksperimentere med mange forskellige l{\o}sninger til dette problem. Hovedfokuset bliver at vise hvor stor forskel der er p{\aa} strukturer optimeret til at h{\aa}ndtere store datam{\ae}ngder og strukturer der er optimeret til at kunne v{\ae}re i intern hukommelse ved at sammenligne dem eksperimentielt.

Vi har igennem specialet implementeret to strukturer optimeret til ekstern hukommelse: Det Eksterne Hukommelses Prioritets S{\o}ge Tr{\ae} af Arge et al.~\cite{arge_samoladas_vitter_1999} og det Eksterne Hukommelses \textit{Bufferet} Prioritets S{\o}ge Tr{\ae} af Brodal~\cite{DBLP:journals/corr/Brodal15}.

Vi har derudover implementeret et Internt Hukommelses Prioritets S{\o}ge Tr{\ae} af McCreight~\cite{DBLP:journals/siamcomp/McCreight85}, og lavet omslag til MySQL, libspatialindex R*Tr{\ae} og Boost R-Tr{\ae}.

Resultater fra eksperimenter viser at det Eksterne Hukommelses Prioritets S{\o}ge Tr{\ae} sl{\aa}r alle de andre data strukturer s{\aa} snart de skal h{\aa}ndtere mere data end der er plads til i computerens interne hukommelse. MySQL uden et tilh{\o}rende indeks var den eneste struktur der kunne hamle op n{\aa}r det kommer til at inds{\ae}tte i strukturen. Denne struktur m{\aa}tte dog betale dyrt for de hurtige inds{\ae}ttelser i form af langsomme slettelser og foresp{\o}rgsler. Det var Boost R-Tr{\ae}et der viste sig at v{\ae}re den hurtigste af de interne hukommelses strukturer.

\todo{conclude more?}

\tableofcontents

\chapter*{Preface}
\label{chp:preface}
\addcontentsline{toc}{chapter}{Preface}

Writing this thesis has been been a long and often frustrating process with many ups and downs. The \textit{struggle} has been very real\footnote{Denotes a situation where the writer wishes to express that they are encountering some sort of undesirable difficulty, but dealing with it~\cite{real_struggle}.} at some points during the four and a half months. We got off to a very shaky start to say the least. Only a few days before we were supposed to begin work on our thesis we decided, based on a gut feeling, that the topic we were initially set on covering was not going to lead to a good thesis; or at least not one that we thought would encapsulate the hard work we have been putting in to our courses in the 5 years we have studied Computer Science at Aarhus University. With much haste, we thus decided to try set up a meeting with Kasper Larsen and Gerth St\o lting Brodal. Gerth referred us to Kasper and to our surprise Kasper had a project in mind. The project sounded extremely challenging but also very interesting. It would allow us to really dig into some data structures of very high complexity and implement these in \texttt{C++} while at the same time make use of our deep understanding of algorithm analysis. This would allow us to utilize all of the hard work we had put into virtually all offered algorithmic courses. We gladly accepted and only a few days before start we now had ourselves a project.
\begin{center}
\texttt{\#BetterGutFeeling}.
\end{center}
After having spent a few days reading through many articles we had a better idea of what we were up against. The coding challenge was very daunting. We initially estimated that we would need about 80\% of the total time just to implement the structures. We were not much off that estimate. We have tried to summarize the main milestones throughout the project in Figure~\ref{fig:git_history}, and we believe it gives a fair overview of how much effort we have put into coding. In the end of the project we had a total of 19,009 lines of \texttt{C++} code.

Even though the project has not been a walk in the park we still managed to have a lot of fun throughout. As we were approaching the phase in the project where we were going to start experimenting, the summer was also nearing and the temperatures came close to the boiling point in our small office. With five machines working overtime to finish experiments it is safe to say that our indoor environment would not live up to human rights regulations but as a computer scientist it was cozy and it was our home for the time being. We put \twonotes~Nelly with \textit{Hot in here} on the stereo and embraced the situation.

The experiments often ran for more than a week at a time and in order for us to know exactly when an experiment had finished, we had the clever idea of using the beep function of the motherboard to alert us. As a simple beep would be too boring we spent some time writing a small piece of code which, by controlling the frequency and duration of the beep sound, would play the Star Wars song: The Imperial March\footnote{You know, the song that comes on when Darth Vader is there.}. All was good and it worked perfectly until one weekend where we did not come in early. As we walked down the corridors approaching our office the sound of Darth Vader became increasingly ear-wrecking. The sound was really annoying and we really hoped that all the other tenants in the building would be home during the weekend. As we walked down the corridor towards our office we looked through the small window next to the door to see if they were noticeably annoyed, but it did not seem to bother them. Just before our office we finally glanced into the office just next to ours and in there sat a very displeased lady with large headphones clearly very annoyed by the deafening sound of The Imperial March. We are truly sorry about this.

Another perk of The Imperial March was the ability to scare the crap out of Christoffer. Peter would be able to sit comfortably at home and randomly start the cacophony which would make Christoffer jump out of his seat and generally become very anxious about when the next sound would come.

After more than a month of intense experimenting, documentation, and \LaTeX'ing we concluded with a small out-of-sight victory dance, shut down our faithful test machines, and with all our new knowledge and insight we could finally have our happily ever after.

\todo{More people to thank?}

A number of people have been part to giving this thesis its life. First and foremost we would like to thank our primary advisor, Kasper Larsen, for our weekly meetings, constructive feedback, enthusiasm, and engagement in the project.
We would also like to thank our formal advisor, Gerth St{\o}lting Brodal, for his great work on the data structure we have implemented in this thesis and sparring when we were in doubt.
In more general terms we would like to thank Aarhus University for giving us 5 years worth of great courses and preparing us for the life to come within the field of Computer Science.

We have really enjoyed working on this thesis and we sincerely hope you enjoy reading it as much as we have enjoyed writing it!

\vspace{2ex}
\begin{flushright}
  \emph{Peter Gabrielsen and Christoffer Holb{\ae}k Hansen}\\
  \emph{Aarhus, \today.}
\end{flushright}

\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{commit}=[draw,circle,fill=white,inner sep=0pt,minimum size=5pt]
\tikzstyle{clabel}=[right,outer sep=1em]
\tikzstyle{every path}=[draw]
\matrix [column sep={1em,between origins},row sep=\lineskip]
{
\commit{45b7bbd}{08.Jun}{All Makefile targets are updated and README changed} & \\
\commit{701f7a9}{07.Jun}{Query experiment results come in} & \\
\commit{52d4e39}{25.May}{Brodal buffer size experiment documented} & \\
\commit{1da6e4b}{10.May}{Preface added} & \\
\commit{47b00ae}{29.Apr}{libspatialindex PST added} & \\
\commit{ed54a60}{21.Apr}{Now plays The Imperial March} & \\
\commit{4ee8a7a}{19.Apr}{Base experiment class added} & \\
\commit{e028c0b}{17.Apr}{Merge: MySQL PST done + Internal PST done} & \\
& & \ghost{branch6} & \commit{f585f41}{14.Apr 2016}{Internal dynamic PST added} \\
& \commit{f96b91a}{12.Apr}{Merge: Boost R-tree done and MySQL PST started} & & \\
& \ghost{branch5} & \commit{1132d2e}{12.Apr 2016}{Documenting theory of Arge} \\
\ghost{branch4} & \commit{ecd8b6e}{11.Apr 2016}{Wrapper of Boost R-tree implemented} \\
\commit{5f2471b}{07.Apr 2016}{Merge: All tests of Arge passes} & \\
\commit{3db0fea}{06.Apr 2016}{All tests of Brodal passes} & \ghost{branch3} \\
& \commit{1e4e94b}{30.Mar 2016}{Implementation of Arge begun} \\
\commit{69296f7}{29.Mar 2016}{Cache event loop and intense bug hunting} & \ghost{branch2} \\
& \commit{ebad13f}{28.Mar 2016}{Documenting theory on Brodal} \\
\commit{6d7910d}{15.Mar 2016}{Total refactoring from recursion to event loop} & \\
\commit{d3857d6}{05.Mar 2016}{Cascading overflowing of point buffers} & \\
\commit{d4db0d5}{24.Feb 2016}{Node degree overflow might work} & \ghost{branch1}\\
\commit{d764b48}{19.Feb 2016}{Implementation of Brodal started} & \\
& \commit{7657914}{18.Feb 2016}{Documenting theory on Child structure} \\
\commit{54ba4b2}{17.Feb 2016}{All tests of Child structure passes} & \\
\commit{c589395}{11.Feb 2016}{Implementation of Child structure started} & \\
\commit{9f9c652}{10.Feb 2016}{All tests of I/O streams passes} & \\
\commit{b3bd158}{08.Feb 2016}{Implementation of I/O streams started} & \\
\commit{63268c1}{01.Feb 2016}{Initial commit}& \\
};
\connect{63268c1}{b3bd158};
\connect{b3bd158}{9f9c652};
\connect{9f9c652}{c589395};
\connect{c589395}{54ba4b2};
\connect{54ba4b2}{d764b48};
\connect{54ba4b2}{7657914};
\connect{7657914}{branch1};
\connect{branch1}{d3857d6};
\connect{d764b48}{d4db0d5};
\connect{d4db0d5}{d3857d6};
\connect{d3857d6}{6d7910d};
\connect{6d7910d}{ebad13f};
\connect{6d7910d}{69296f7};
\connect{ebad13f}{branch2};
\connect{branch2}{3db0fea};
\connect{69296f7}{1e4e94b};
\connect{1e4e94b}{branch3};
\connect{branch3}{5f2471b};
\connect{3db0fea}{5f2471b};
\connect{69296f7}{3db0fea};
\connect{5f2471b}{ecd8b6e};
\connect{5f2471b}{branch4};
\connect{branch4}{f96b91a};
\connect{branch4}{1132d2e};
\connect{f96b91a}{f585f41};
\connect{ecd8b6e}{branch5};
\connect{branch5}{f96b91a};
\connect{f96b91a}{e028c0b};
\connect{f585f41}{e028c0b};
\connect{1132d2e}{branch6};
\connect{branch6}{e028c0b};
\connect{e028c0b}{4ee8a7a};
\connect{4ee8a7a}{ed54a60};
\connect{ed54a60}{47b00ae};
\connect{47b00ae}{1da6e4b};
\connect{1da6e4b}{52d4e39};
\connect{52d4e39}{701f7a9};
\connect{701f7a9}{45b7bbd};
\end{tikzpicture}
\caption{Excerpt of the git history for the entire project. The figure is read bottom up.}
\label{fig:git_history}
\end{figure}

\begin{savequote}[0.7\textwidth]
``The difference in speed between modern CPU and disk technologies is analogous to the difference in speed in sharpening a pencil using a sharpener on one's desk or by taking an airplane to the other side of the world and using a sharpener on someone else's desk.''
\qauthor{--- D. Comer}
\end{savequote}
\chapter{Introduction}
\label{chp:introduction}
\pagenumbering{arabic}
\setcounter{secnumdepth}{2}

% words: big data, banking, data larger than memory, database community, slow disks, disparity between growth of CPU speeds and transfer speeds between internal and external memory (widening gap)
% fault tolerance and consistency are more challenging to handle in-memory (In-Memory Big Data Management and Processing: A survey)
% storage prices
% The I/O bottleneck
% Disk were faster than CPU in 60
% http://read.cs.ucla.edu/111/2006fall/notes/lec15#incommensurate-scaling

In the early days of electronic computers disks were faster than processors. Since then processor technology has advanced at an incredible rate achieving annual speedups of 40 to 60 percent~\cite{ruemmler_wilkes_1994}. Although this is also true for disk capacity an entirely different story can be told for the speed-up of disk performance. The disparity between processor, internal memory, and external memory speeds have grown larger for each year and the gap is widening as seen in Figure~\ref{fig:cpu_vs_disk}. A back on the envelope analysis shows that a job that was 5\% disk bound in 1999 is more than 70\% disk bound on an average CPU in 2014. While the database community has always been involved in the development of practically efficient external memory data structures, most algorithms research has focused on worst-case efficient internal memory data structures~\cite{ionote}. 

\begin{figure}[h]
	\centering
		\includegraphics[width=0.9\textwidth]{../plots/scaling_discrepancy_hdd_cpu/scaling_discrepancy_hdd_cpu}
	\caption{Growth of CPU and HD speed ratio over time.}
	\tiny{Data from~\protect\url{https://en.wikipedia.org/wiki/Instructions_per_second}.}
	\label{fig:cpu_vs_disk}
\end{figure}

With the advent of \textit{Big Data} many industries has come to realise that adapting classic and well founded internal memory algorithms on large data sets is in many cases undesirable. 
They simply prove to perform much slower than the asymptotic bounds suggests. The algorithms community has found the reason to be the very same that ensured the success of the computer industry. The problems derives from the standard RAM-model of computation, where we assume an infinite memory and uniform access cost. See Figure~\ref{fig:ram_model}.

\begin{wrapfigure}{O}{0.5\textwidth}
\captionsetup{width=0.40\textwidth}
	\centering
		\includegraphics[width=0.4\textwidth]{../figures/ram_model}
	\caption{RAM-model. The standard model of computation. We assume an infinite memory with uniform access cost.}
	\label{fig:ram_model}
\end{wrapfigure}

The RAM-model is as powerful as it is simple, but it ignores the more complicated memory hierarchy on modern computers. See Figure~\ref{fig:real_computer}. 

\begin{figure}
	\centering
		\includegraphics[width=0.7\textwidth]{../figures/real_computer}
	\caption{Hierarchical memory. Modern machines have complicated memory hierarchy consisting of registers in the CPU, multi-tier caches (here denoted L1 and L2), volatile main memory and typically a mechanical or solid state disk as external memory.}
	\label{fig:real_computer}
\end{figure}

As we move away from the CPU the access time gets bigger. CPU registers can be accessed in a few nanoseconds. Accessing CPU caches adds a small multiple to that time. Main memory access are typically a few tens of nanoseconds. Now comes a big gap, as the time to access disks are typically measured in milliseconds, i.e. more than $10^6$ times slower than main memory access. Also, the storage capacity increases. CPU registers are good for bytes of data, caches for a few megabytes, main memory for gigabytes, and disks are good for terabytes of data~\cite{Tanenbaum:1998:SCO:552473}.

Disk systems try to amortize the large access time by transferring large contiguous blocks of data and many modern operating systems utilizes sophisticated paging and pre-fetching strategies to move blocks to and from disk as needed~\cite{Tanenbaum:2007:MOS:1410217}. This is the main reason we still have many worst-case optimal internal memory algorithms performing well on large datasets. If the algorithms, however, relies on scattered access across data, even good operating systems cannot take advantage of block access and we start to see severe scalability problems. It is these observations that gives rise to the external memory model. The model encapsulates performance as the number of disk accesses as opposed to RAM accesses. For algorithms analysed in the I/O model it is of extreme importance to store and access data in such a way we can take advantage of data's locality in order to achieve good bounds.

In some industries, disk-based systems presents too large of an obstacle, and in an attempt to close the gap they have moved towards developing internal memory big data processing algorithms. This move has been enabled by growing main memory capacities but it comes with the price of issues such as fault-tolerance and consistency which are inherently more challenging to handle in volatile memory~\cite{Zhang2015}.

Another price of the move to internal memory is the actual cost of running server farms and the cost of internal memory compared to external memory. The extra costs and increased complexity suggests that external memory data structures have some well defined advantages.

Along with pervasive use of computers and sensors, increased ability to acquire and store data, and the society being increasingly \textit{data driven}, it seems that data is collected everywhere today. It is claimed in~\cite{economist:0210} that the amount of generated data on a world-wide scale grew from 150 billion gigabytes to 1200 billion gigabytes from the year 2005 to 2010. This suggest that we, today, generate as much data in a single day as the entire mankind did until 2003.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.9\textwidth]{../plots/massive_data/massive_data}
	\caption{Total amount of data generated by Man over time.}
	\tiny{Data from UC Berkeley.}
	\label{fig:massive_data}
\end{figure}

An industry that has benefit severely from the continuous improvement of technology is that of processing geographic data. Such systems is also known as geographical informations systems (GIS). In the year 2000 the Shuttle Radar Topography mission set out to map Europe and North America in a 30-meter dataset. Denmark alone consists of more than 46 million data points and is stored using gigabytes of data that can easily fit in a modern computers main memory. Today, the dataset has improved to a $\nicefrac{1}{2}$-meter model of more than 168 billion data points. This amounts to terabytes of data that is unlikely to fit into main memory of a standard personal computer in the coming years. Most GIS applications today use results from the field of computational geometry, and it is in this field we will focus our studies.

An integral problem of computational geometry is that of range searching. In addition to GIS applications, the problem arises in many different applications with huge data sets such as spatial databases and computer graphics. The problem can be formally described as follows. Let $\mathcal{S}$ be a set of $N$ points in $\mathbb{R}^d$, and let $\mathcal{R}$ be a family of subsets of $\mathbb{R}^d$. Our objective is to preprocess $\mathcal{S}$ such that for a query range $r \in \mathcal{R}$, the points in $\mathcal{S} \cap r$ can be reported or counted efficiently~\cite{Agarwal99geometricrange}. The ranges can be anything from rectangles and halfspaces to balls.

In this thesis we consider the problem of maintaining a dynamic set, $\mathcal{S}$, of $N$ points in $\mathbb{R}^2$ in external memory. The set of points can be updated by insertion and deletion. The set will be processed such that we are able to report three-sided range queries, i.e. given a range of the type $[x_1,x_2] \times [y,\infty]$, we report points in $\mathcal{S} \cap [x_1,x_2] \times [y,\infty]$.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1]{../figures/three-sided-query}
	\caption{A query of the form $[x_1,x_2] \times [y,\infty]$, reporting all points in the grey area.}
	\label{fig:three-sided-query}
\end{figure}
 
The thesis will present several solutions to this problem, implement, and experimentally compare solutions.

Some of the solutions will not be optimized for usage in external memory and they must pay a hefty price for accessing data not readily available in main memory. This thesis will show the power of the I/O efficient data structures for large data sets and demonstrate what happens when internal memory data structures have to work overtime along with the virtual memory system.
\clearpage
\section{Outline of thesis}
This thesis is structured as follows:

In Chapter~\ref{chp:iomodel} we investigate the I/O model -- a model of computation that encapsulates performance of the I/O bottleneck.

In Chapter~\ref{chp:prelims} we give a preliminary overview of some of the techniques used in developing external memory efficient data structures.

Much work has been done on the three sided range queries and more general range queries. Some of the main ideas leading up to the main focus of this thesis is presented in Chapter~\ref{chp:related_work}.


Chapter~\ref{chp:internal_pst} gives a detailed description and analysis of the Priority Search Tree for internal memory by McCreight~\cite{DBLP:journals/siamcomp/McCreight85}.

Chapter~\ref{chp:arge_pst} presents a result by Arge et al~\cite{arge_samoladas_vitter_1999} with optimal query bounds.

The main focus of this thesis, the External Memory Buffered Priority Search Tree of Brodal~\cite{DBLP:journals/corr/Brodal15} is presented in Chapter~\ref{chp:epst}.

Leading up to our presentation of our experimental results in Chapter~\ref{chp:experimental_results} we present other structures included in our experiments in Chapter~\ref{chp:other_structures}, considerations throughout our implementation in Chapter~\ref{chp:implementation}, and our experimental setup in Chapter~\ref{chp:experimental_setup}.

We conclude the thesis in Chapter~\ref{chp:conclusion} and present possible future work that could be very interesting to see.


\begin{savequote}[0.5\textwidth]
``All models are wrong but some are useful.''
\qauthor{---  George E. P. Box}
\end{savequote}
\chapter{Model of computation}
\label{chp:iomodel}
We will argue the results of this thesis in terms of the external memory model of Aggarwal and Vitter~\cite{Aggarwal:1988/ICS/48529.48535}.
The external memory model (or I/O model) measures the efficiency of an algorithm by counting the total number of reads and writes to and from disk. In detail the model consists of two levels of memory; a bounded internal memory of size $M$ and an unbounded external memory. For a total of $N$ records we define an I/O operation to be the process of transferring $B$ contiguous records between the two levels of memory as depicted in Figure~\ref{fig:io_model}. We restrict all computations on records to be done in internal memory. Throughout the thesis we will let $K$ denote the total number of records in the output.

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{../figures/block_io}
	\caption{The I/O Model. Only reads and writes of contiguous blocks between internal and external memory are charged.}
	\label{fig:io_model}
\end{figure}

The fundamental bounds in the external memory model is that scanning $N$ records can be done in $\mathcal{O}(\text{Scan}) = \mathcal{O}(\nicefrac{N}{B})$, sorting $N$ records in $\mathcal{O}(\text{Sort}) = \mathcal{O}\left(\frac{N}{B} \log_{\nicefrac{M}{B}}\frac{N}{B}\right)$, and searching for a single record between $N$ records in $\mathcal{O}(\log_B N)$. We denote $\mathcal{O}(\nicefrac{N}{B})$ as being linear in terms of I/O's. Note that the $B$ factor is very important since $\nicefrac{N}{B} < \mathcal{O}\left(\frac{N}{B} \log_{\nicefrac{M}{B}}\frac{N}{B}\right) \ll N$.

For convenience we will assume $M > B^2$. This assumption is known as the \textit{tall-cache assumption} in the cache-oblivious model and basically states that the number of blocks \nicefrac{M}{B} is larger than the size of each block $B$~\cite{Prokop99cache-obliviousalgorithms}.

\begin{savequote}[0.42\textwidth]
``The way I see it, if you want the rainbow, you gotta put up with the rain.''
\qauthor{--- Dolly Parton}
\end{savequote}
\chapter{Preliminaries}
\label{chp:prelims}
This chapter aims to give an overview of some of the techniques and structures used throughout the thesis. Some of the techniques are very rudimentary and may be skipped or revisited when encountered in later chapters.

\section{Amortization}
Amortization is an important algorithmic tool to argue about average performance of an operation in the worst case.
In an amortized analysis, we average the time of a sequence of data structure operations. We can then show that, even though a single operation in the sequence is expensive, the average cost of an operation is small~\cite[p.~451-452]{clrs}.

The term was coined by Tarjan~\cite{Tarjan85} and describes two views of amortization. The first view is the banker's view where we assume that a computer is running on coins. We can insert a coin and the computer will run for a fixed constant amount of time. An operation will pay a certain amount of coins and the goal of the analysis is then to show that all operations can be performed with the amount paid. We assume that we start without any coins, we are allowed to borrow coins, and coins can be carried over to later operations. Paying coins amounts to averaging forward over time and borrowing is the opposite.

Another view of amortization is that of the physicist. Instead of representing prepaid work as coins the physicist represent work as potential energy which can be released later to pay for future operations.

If we perform $n$ operations, we will start with an initial data structure $D_0$. For each of the operations we let $c_i$ be the cost of operation $i$ and $D_i$ be the data structure that results from that operation on the previous data structure. We define a potential function $\Phi$ to map a data structure $D_i$ to a real number $\Phi(D_i)$. The cost of the $i$'th operation becomes
$$\hat{c}_i = c_i + \Phi(D_i) - \Phi(D_{i-1})$$
The total amortized cost becomes
\begin{align*}
\sum_{i=1}^n \hat{c}_i &= \sum_{i=1}^n c_i + \Phi(D_i) - \Phi(D_{i-1}) \\
&= \sum_{i=1}^n c_i + \Phi(D_n) - \Phi(D_{0})
\end{align*}

If we can show $\Phi(D_i) \geq \Phi(D_0)$ for all $i$ then we know that we always are able to build up enough potential in advance.

\section{Global rebuilding}
\label{sec:prelim_global_rebuilding}
The term \textit{global rebuilding} refers to the technique of making a (typically small) static data structure dynamic. We simply store all updates in an \textit{update block} and once a certain threshold has been collected we rebuild the data structure~\cite{ionote}. For data structures that does not allow the space for deleted records to be reoccupied we \textit{mark} (or \textit{weak delete}) the elements. Whenever $\alpha N$ elements have been marked, for some constant $\alpha > 0$, the entire data structure is rebuilt from scratch with only the non-marked elements. The cost of rebuilding is at most a constant factor higher than the cost of inserting $\alpha N$ elements and so the amortized cost of global rebuilding is paid in advance when elements are inserted, i.e. elements are charged double such that they later can pay for being removed from the structure during a global rebuild.~\cite{Meyer:2003/AMH/1744652}.

\section{Filtering}
\label{sec:filtering}
The technique of \textit{filtering} is used on retrieval problems where we query a certain data structure for a subset of data points. The technique is based on the fact that the complexity of the \textit{search} and the \textit{report} parts of the algorithm should be made dependent upon each other such that we charge part of the query cost to output. In order to make filtering feasible, it is crucial that the problems specifically require the exhaustive enumeration of the objects satisfying the query.

\section{Bootstrapping}
It is often possible to develop dynamic external memory data structures by "externalizing" the equivalent internal memory data structure. In the case of trees this typically involves increasing the fanout from binary to multiway. This, however, results in problems when searching and reporting items from the tree, e.g. it might be the case that each subtree of a node only contributes one item to the query answer each costing one I/O.

This problem can sometimes be solved by augmenting the data structure with several filtering substructures, i.e. smaller versions of the same problem. This approach was first described by Arge and Vitter~\cite{arge_vitter_2003} in a paper giving an optimal solution to diagonal corner two-sided 2D queries. Each of the substructures typically holds $\mathcal{O}(B^2)$ elements ($B$ elements from each subtree), and answers queries in $\mathcal{O}\left(\log_B B^2 + \nicefrac{K}{B}\right)$, where $K$ is the size of the output. It can even be a static structure if it can be constructed in $\mathcal{O}(B)$ I/O's, since $B$ updates can be stored in a separate buffer and applied using a global rebuild in amortized $\mathcal{O}(1)$ I/O's per update~\cite{vitter_2008}.

\section{B-Tree}
\label{sec:prelim_b_tree}
The B-Tree of Bayer and McCreight~\cite{bayer_mccreight_1972} is to external memory what the balanced binary search tree is to internal memory. It supports insertions and deletions of points, and searching in $\mathcal{O}(h)$ where $h$ is the height of the tree. The height of the tree depends on the branching parameter, i.e. the maximum number of children a node can have. This parameter typically depends on the characteristics of the disk used and the problem at hand. 
This gives the following definition of a B-Tree:

\begin{definition}
\label{def:btree}
$\mathcal{T}$ is a B-Tree with branching parameter $b$ if
\begin{itemize}
	\item All leafs have the same depth.
	\item All nodes store at most $b-1$ elements.
	\item All nodes and leafs except for the root have degree between $\frac{1}{2}b$ and $b$.
	\item The root has degree between $2$ and $b$.
	\item Elements are stored in non-decreasing order in the nodes.
	\item The keys of node $x$, $x.key_i$, separate the children's elements into ranges such that if $k_i$ is a key stored in child $c_i$ then $k_1 \leq x.key_1 \leq k_2 \leq x.key_2 \leq \cdots$
\end{itemize}
\end{definition}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{../figures/b-tree}
	\caption{A B-tree with $b = 4$.}
	\label{fig:b-tree}
\end{figure}

It follows from the definition that if $b=\Theta(B)$ then a B-Tree will have height $\mathcal{O}(\log_B N)$. Note that the B-Tree is a special case of the $(a,b)$-trees in which the number of elements in leafs is a uniquely defined parameter.

\textbf{Searching} in a B-Tree is very much similar to searching in a binary search tree. Instead of making a binary decision at each node we instead have to make a multiway branching decision. If the element we are searching for is not contained in the current node, we find the smallest $i$ such that the key we are searching for is less than $x.key_i$. We then recursively search for the key in child $c_i$. This will in the worst case require $\mathcal{O}(\log_B N)$ I/O's to search for an element residing in a leaf.

\textbf{Inserting} in a B-Tree is not as simple as inserting into a binary search tree. Similarly to binary trees we search for the leaf node to insert the key, but we cannot simply create a new node for the key. Instead we insert the key into the found leaf node and if the leaf now contains too many elements we split the leaf into two leafs each containing half the elements of the original leaf. The median of the elements are inserted in the parent. See Figure~\ref{fig:b_tree_split}. Splitting a leaf might cause its parent to have too many children which causes the parent to similarly split.
Searching for the leaf node to insert into takes $\mathcal{O}(\log_B N)$ and so does recursively splitting nodes from a leaf to root path as a split operation requires $\mathcal{O}(1)$ I/O's.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{../figures/b_tree_split}
	\caption{Splitting a degree $b+1$ node $v$ (or leaf with $b+1$ elements) into nodes (or leafs) $v'$ and $v''$.}
	\label{fig:b_tree_split}
\end{figure}

\textbf{Deleting} in a B-Tree introduces an opposite to splitting, fusing. To delete a key from the tree we search the tree for the key, which now can reside in an internal node $x$. We then delete the key from the node $x$, which might cause $x$ to have too few elements. To remedy this situation we will have to potentially fuse $x$ with a neighbouring node. If $x$ together with either its predecessor or successor contains less than $b$ elements we can fuse the two nodes. See Figure~\ref{fig:b_tree_fuse}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{../figures/b_tree_fuse}
	\caption{Fusing a degree $(b/2)-1$ node $v$ (or leaf with $(b/2)-1$ elements) with sibling $v'$.}
	\label{fig:b_tree_fuse}
\end{figure}

If this is not the case then we know that we are able to steal an element from a neighbouring node in order to satisfy the properties of the B-Tree. As in the case of insertion, fusing nodes might recursively cause the parent to fuse with one of its neighbours.
Fusing two nodes require $\mathcal{O}(1)$ I/O's but a fuse can cascade from a leaf to root path causing $\mathcal{O}(\log_B N)$ I/O's.

A B-Tree on $N$ elements are stored in $\mathcal{O}(\nicefrac{N}{B})$ blocks and can be constructed in the sorting bound by building the tree level-by-level bottom-up.

\subsection{Weight-balanced B-Tree}
A weight balanced B-Tree and a regular B-Tree differ with regards to how and when splitting and fusing of nodes takes place. Constraints are imposed on the \textit{weight} of a node rather than the number of children. The weight of a node $v$ is the number of elements stored in the subtree rooted at $v$. Let $a$ be the branching parameter and $k$ the leaf parameter of the tree. An internal node on level $l$ has weight between $\frac{1}{2}a^lk$ and $2a^lk$ and has at least one child. Inserting in a weight balanced B-Tree is similar to a normal B-Tree and when a leaf splits it might cause the weight of the parent to become too large and recursively split on a path from a leaf to the root.

The strength of the weight-balanced B-Tree is the crucial property described in the following lemma.

\begin{lemma}
\label{lma:weight_balanced}
After a split of a node $v_l$ on level $l$ into two nodes $v_l'$ and $v_l''$, at least $a^lk/2$ inserts have to pass through $v_l'$ (or $v_l''$) to make it split again. After a new root $r$ in a tree containing $N$ items is created, at least $3N$ inserts have to be done before $r$ splits again.
\end{lemma}

The proof, simply put, states that a node $v$ will not underflow or overflow unless $\Omega(weight(v))$ leaf elements have been inserted or deleted in the subtree of $v$. A proof of the lemma can be found in~\cite{arge_vitter_1996}.
\clearpage
\section{Buffer Tree}
\label{sec:prelim_buffer_tree}
The Buffer Tree of Arge~\cite{Arge:1995:BTN:645930.672850} combines the basic B-Tree described in Section~\ref{sec:prelim_b_tree} with a \textit{buffer-technique} that will be introduced shortly. The result is an external data structure supporting batched operations efficiently in terms of I/O's. The ideas introduced by Arge has proven especially useful when generalizing well-known internal-memory algorithms into efficient I/O algorithms~\cite{Arge:1995:BTN:645930.672850}. The main idea of the buffer-technique is to introduce \textit{laziness} in the update algorithms and utilize main memory to process a large number of updates simultaneously. For example, when inserting a point we do not search all the way down the tree to find the leaf. Instead, the point is inserted into a buffer of the root. Whenever the size of a buffer exceeds a certain threshold we push elements from the buffer one level down to buffers on the next level of the tree. This process of emptying full buffers is repeated recursively down the tree.

Formally the basic Buffer Tree is defined according to Definition~\ref{def:buffer_tree}. Refer to Figure~\ref{fig:buffer_tree} for an illustration of a Buffer Tree.

\begin{definition}
\label{def:buffer_tree}
A basic Buffer Tree $\mathcal{T}$ is
\begin{itemize}
	\item A B-Tree with leaf parameter $B$.
	\item All internal nodes, except for the root, have degree between $\frac{1}{4}$ $\nicefrac{M}{B}$ and $\nicefrac{M}{B}$.
	\item The root have degree between 2 and $\nicefrac{M}{B}$.
	\item Each internal node has a buffer of size $M$.
\end{itemize}
\end{definition}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{../figures/buffer_tree}
	\caption{Buffer Tree.}
	\label{fig:buffer_tree}
\end{figure}

\textbf{Updates} are handled by augmenting the element in question with information on whether we are inserting or deleting the element. Since an element can be represented in multiple buffers, we also augment elements with a time stamp.
Whenever we have collected $B$ elements we insert all of them into the root buffer of size $M$. Whenever the buffer overflows, i.e. have more than $M$ points, we initiate a buffer-emptying process that distributes all elements in the buffers to the children.

For an \textit{internal} node that does not have leafs as children this process is done as follows. First, we load the $M$ unsorted elements into main memory and sort them. Then we scan through the sorted list while removing matching inserts and deletes with respect to the time stamps of each element. Now we simply distribute the remaining elements one level down using a single scan. We make sure to distribute the elements in sorted order, since this will guarantee that we leave no buffer of a child with more than $M$ unsorted elements followed by a list of sorted elements. Thus, we are able to sort the resulting buffer in a linear number of I/O's as depicted in Figure~\ref{fig:buffer_tree_buffer_sort}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/buffer_tree_buffer_sort}
	\caption{(a) First, the $\leq M$ unsorted points are loaded into main memory in $\mathcal{O}(M/B)$ I/O's and are then sorted in internal memory (b) Then, the two lists of sorted points are merged in $\mathcal{O}(M/B)$ I/O's.}
	\label{fig:buffer_tree_buffer_sort}
\end{figure}

We then recursively empty full child buffers provided that the children are internal nodes that do not have leafs as children. Only when we have emptied buffers of all overflowing internal nodes which do not have leafs as children, we proceed the buffer-emptying process to leaf nodes. The reason is that a buffer-emptying process on a leaf node may result in the need for rebalancing. By only emptying leaf nodes after all internal node buffer-emptying processes have been performed we prevent rebalancing and buffer-emptying processes from interfering with each other.

We empty all relevant leaf nodes buffers one-by-one while maintaining the \textit{leaf-emptying invariant} that all buffers of nodes on the path from the root of $\mathcal{T}$ to a leaf node with a full buffer are empty. Since we handle all internal nodes before emptying the leaf nodes this invariant is true when we handle the buffer-emptying of the first leaf. To empty a node $u$ with $K$ elements in the leafs, we start by sorting the buffer and remove matching inserts and deletes. Then, we merge the buffer elements with the $K$ leafs below, again removing matching inserts and deletes. The resulting set of $K'$ sorted elements now needs to replace the $K$ original leafs along with new routing elements of $u$ reflecting the changes. If we end up with a resulting set of size $K' < K$, i.e. we do not have enough elements to fill the $K$ leafs, we introduce $K-K'$ dummy elements and insert those in the remaining leafs. If we have $K' \geq K$ we place $K$ elements in the leafs. The remaining elements (if any) are finally placed one-by-one while issuing rebalancing of the B-Tree when necessary. See Figure~\ref{fig:buffer_tree_buffer_empty}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.73\textwidth]{../figures/buffer_tree_buffer_empty}
	\caption{Buffer emptying process. First the $K$ original leafs and the buffer are merged, and matching inserts and deletes are removed. This gives a set of size $K'$. If $K = K'$ the original leafs are replaced with the $K'$ new ones. If $K' < K$ we add \textit{dummy elements}, here represented as circles, to the set such that we replace all of the original $K$ leafs with elements from the new set. If $K' > K$, we use a subset of size $K$ to replace the original leafs, and the rest of the points are then inserted one by one.}
	\label{fig:buffer_tree_buffer_empty}
\end{figure}

We can rebalance as in a normal B-Tree using splits as depicted in Figure~\ref{fig:buffer_tree_split}, since the leaf emptying invariant ensures that all nodes from $u$ to the root of $\mathcal{T}$ have empty buffers.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/buffer_tree_split}
	\caption{Split in a Buffer Tree. The \textit{leaf-emptying invariant} guarantees that there are empty buffers on the root to leaf path ensuring splits can be done as in a normal B-Tree.}
	\label{fig:buffer_tree_split}
\end{figure}

After we have emptied all leaf-node buffers we remove the place-holder elements one-by-one. The leaf-emptying invariant ensures that a node $v$ on the path from $u$ to the root has an empty buffer, but $v's$ sibling may not have an empty buffer. Therefore we cannot fuse in a normal B-Tree manner. Instead, we perform a buffer-emptying process on $v's$ immediate sibling before performing the actual fuse.
The emptying of the buffer of a sibling node $v'$ can result in buffers running full. We empty all such full non-leaf buffers before performing the actual fuse on $v$. See Figure~\ref{fig:buffer_tree_fuse}.

The place-holder elements ensures we are always only in the process of handling a rebalancing caused by a single delete.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{../figures/buffer_tree_fuse_v2}
	\caption{Buffer tree fusing. (a) node $v$ already have an empty buffer guaranteed by the \textit{leaf-emptying invariant}, but no such guarantee is given for the sibling node $v'$, so we have to start a buffer-emptying process on $v'$ (b) The buffer emptying of $v'$ might cause a leaf-node split which is handled before we fuse $v$ and $v'$ (c) After the buffer-emptying process of $v'$ and rebalancing has finished we perform the actual fuse of nodes. This is done as in a normal B-Tree.}
	\label{fig:buffer_tree_fuse}
\end{figure}

\subsection{Analysis}
To empty an internal node buffer of size $X \geq M$ we need $\mathcal{O}(\nicefrac{X}{B})$ to scan the elements and $\mathcal{O}(\nicefrac{M}{B})$ to distribute them one level down. In order to empty a leaf node we have to scan the $\Theta(M)$ elements below it which gives an additional $\mathcal{O}(\nicefrac{X}{B} + \nicefrac{M}{B})$ I/O's.

By letting the branching parameter equal $\nicefrac{M}{B}$ and the leaf parameter equal $B$, we can push all elements in a buffer of size $M$ down to the next level in $\mathcal{O}(\nicefrac{M}{B})$ I/O's. This follows from the fact that all the elements fit into main memory and we use $\mathcal{O}(1)$ I/O's to push one block one level down. Disregarding rebalancing of the tree, we can argue that we touch each block of elements a constant number of times on each of the $\mathcal{O}\left(\log_{\nicefrac{M}{B}} \frac{N}{B}\right)$ levels of the tree. Thus, inserting $N$ elements can be done in an optimal $\mathcal{O}\left(\frac{N}{B} \log_{\nicefrac{M}{B}} \frac{N}{B}\right)$ I/O's  assuming no rebalancing of the tree.

It is showed in~\cite{ionote} that a $N$-element B-Tree with branching parameter $b$ and leaf parameter $k = \Omega(B)$ has an amortized number of internal node rebalancing operations (split/fuse) needed after an update equal to $\mathcal{O}\left(\frac{1}{b \cdot k}\log_b \frac{N}{B}\right)$ I/O's. It follows directly that the total number of internal node rebalancing operations performed during $N$ updates is $\mathcal{O}\left(\frac{N}{b \cdot M/B}\log_{M/B} \frac{N}{B}\right)$ Since each operation takes $\mathcal{O}(\nicefrac{M}{B})$ I/O's to empty a non-empty buffer, the total cost of the rebalancing is also $\mathcal{O}\left(\frac{N}{B} \log_{\nicefrac{M}{B}} \frac{N}{B}\right)$.

We conclude that the total cost of a sequence of $N$ update operations on an initially empty Buffer Tree is $\mathcal{O}\left(\frac{N}{B} \log_{\nicefrac{M}{B}} \frac{N}{B}\right)$.


\begin{savequote}[0.5\textwidth]
``Nanos gigantum humeris insidentes.''
\qauthor{--- Bernard of Chartres}
\end{savequote}
\chapter{Related work}
\label{chp:related_work}
McCreight introduced the Priority Search Tree for internal memory in~\cite{DBLP:journals/siamcomp/McCreight85}. The Priority Search Tree is basically a combination of a binary search tree on the $x$-coordinate and a heap on the $y$-coordinate, where the root of every subtree stores the maximum $y$-value in that subtree and points are distributed according to the median $x$-value. This allows updates to be done in $\mathcal{O}(\log N)$ time and three-sided range queries to be done in $\mathcal{O}(\log N + K)$ time, where $K$ is the number of points being reported. The Internal Memory Priority Search Tree is explained in greater detail in Chapter~\ref{chp:internal_pst}.

The study of adapting the Priority Search Tree to external memory was initiated by Icking et al.~\cite{Icking1988}. They achieve a static external Priority Search Tree that uses $\mathcal{O}(\nicefrac{N}{B})$ space and answers three-sided range queries in $\mathcal{O}(\log_B N + \nicefrac{K}{B})$ I/O's. The data structure uses a blocked B-Tree with pointers to data buckets full of points. The idea is depicted in Figure~\ref{fig:icking_external_pst}. In order to make the data structure dynamic the underlying B-Tree is replaced with a Red-Black tree. This change of underlying search tree results in a solution that answers queries in $\mathcal{O}(\log_2 N + \nicefrac{K}{B})$ I/O's and handles updates in $\mathcal{O}(B \log_2 N)$ I/O's.

\begin{figure}[b]
	\centering
		\includegraphics[width=0.83\textwidth]{../figures/icking_external_pst}
	\caption{Illustration of the solution of Icking et al. Blocked B-Tree with pointers to full buckets of data points.}
	\label{fig:icking_external_pst}
\end{figure}

Kanellakis et al. presents a linear space and partially dynamic solution in~\cite{Kanellakis1996589}. The data structure answers three-sided queries in $\mathcal{O}(\log_B N + \nicefrac{K}{B} + \log_2 B)$ I/O's and supports inserts in $\mathcal{O}(\log_B N + (\log^2_B \nicefrac{N}{B})$ I/O's. The result is fairly involved and is unlikely to perform well in any practical manner. Please refer to Appendix~\ref{chp:kanellakis} for a presentation of the overall ideas of their solution.

Ramaswamy and Subramanian presents a suboptimal space data structure that answers three-sided queries with an optimal query bound in \cite{Ramaswamy:1994:PCT:182591.182595}. They use the same basic blocked B-Tree with pointers to full buckets of data points as introduced by Icking et al.~\cite{Icking1988} and illustrated in Figure~\ref{fig:icking_external_pst}. In addition they introduce the idea of \textit{path caching}. Please refer to Appendix~\ref{chp:ramaswamy} for a presentation of the main ideas of their solution.

Ramaswamy and Subramanian continues their work and brings down the space usage in~\cite{Subramanian:1995:PTN:313651.313769}. This is done by building a search tree that divides the points into smaller regions and using a slightly modified caching scheme. Further details of the main ideas can be found in Appendix~\ref{sec:ramaswamy_better_space}.

Arge et al. presented the first linear space dynamic data structure with optimal query bounds and suboptimal update bounds in~\cite{arge_samoladas_vitter_1999}. The data structure supports queries using $\mathcal{O}(\log_B N + \nicefrac{K}{B})$ I/O's and updates using $\mathcal{O}(\log_B N)$ I/O's. Please refer to Chapter~\ref{chp:arge_pst} for a detailed description of the solution.

Brodal~\cite{DBLP:journals/corr/Brodal15} introduced an amortized solution that improves the update bound of~\cite{arge_samoladas_vitter_1999} by a factor $\epsilon B^{1-\epsilon}$ by adding $\epsilon^{-1}$ to the query bound. This gives a data structure supporting updates in amortized $\mathcal{O}\left(\nicefrac{1}{\epsilon B^{1-\epsilon}} \log_B N\right)$ and three-sided range queries in amortized $\mathcal{O}\left(\nicefrac{1}{\epsilon}\log_B N + K/B\right)$ for $0 < \epsilon \leq 1$. The data structure adapts ideas of the Buffer Tree of Arge~\cite{Arge:1995:BTN:645930.672850} to the External Memory Priority Search Tree of Arge~\cite{arge_samoladas_vitter_1999}. The solution is presented in detail in Chapter~\ref{chp:epst}.

Please refer to Table~\ref{tbl:related_work_summary} for a summary of the results.

\begin{table}[t]
\centering
\caption{Previous dynamic external-memory three-sided range reporting data structures. All query bounds except for~\cite{Subramanian:1995:PTN:313651.313769} are optimal. Amortized bounds are marked \textdagger, and $\epsilon$ is satisfying $0 < \epsilon \leq 1$. All data structures requires $\mathcal{O}(\nicefrac{N}{B})$ space, except for~\cite{Ramaswamy:1994:PCT:182591.182595} requiring space $\mathcal{O}(\nicefrac{N}{B} \log_2 B \log \log B)$. $\mathcal{IL^*}(x)$ denotes the number of times $\log^*$ must be applied before the result becomes $\leq 2$}
\label{tbl:related_work_summary}
\centerline{
\begin{tabular}{llll}
\multicolumn{1}{c}{\small{Reference}} & \multicolumn{1}{c}{\small{Update}} & \multicolumn{1}{c}{\small{Query}} & \multicolumn{1}{c}{\small{Construction}} \\ \hline
\multicolumn{1}{c}{\small{\cite{Ramaswamy:1994:PCT:182591.182595}}} & \multicolumn{1}{c}{\small{$\mathcal{O}\left(\log N \log B\right)^{\text{\textdagger}}$}} & \multicolumn{1}{c}{\small{$\mathcal{O}\left(\log_B N + K/B\right)$}} &  \\
\multicolumn{1}{c}{\small{\cite{Subramanian:1995:PTN:313651.313769}}} & \multicolumn{1}{c}{\small{$\mathcal{O}\left(\log_B N + (\log_B N)^2 / B\right)^{\text{\textdagger}}$}} & \multicolumn{1}{c}{\small{$\mathcal{O}\left(\log_B N + K/B + \mathcal{IL}^*(B)\right)$}} & \\
\multicolumn{1}{c}{\small{\cite{Arge:1995:BTN:645930.672850}}} & \multicolumn{1}{c}{\small{$\mathcal{O}\left(\log_B N\right)$}} & \multicolumn{1}{c}{\small{$\mathcal{O}\left(\log_B N + K/B\right)$}} & \\
\multicolumn{1}{c}{\small{\cite{DBLP:journals/corr/Brodal15}}} & \multicolumn{1}{c}{\small{$\mathcal{O}\left(\frac{1}{\epsilon B^{1-\epsilon}} \log_B N\right)^{\text{\textdagger}}$}} & \multicolumn{1}{c}{\small{$\mathcal{O}\left(\frac{1}{\epsilon}\log_B N + K/B\right)^{\text{\textdagger}}$}} & \multicolumn{1}{c}{\small{$\mathcal{O}\left(\text{Sort}(N)\right)$}} \\ \hline
\end{tabular}
}
\end{table}

\section{Lower bounds}	
Solving the problem of three-sided range queries is very closely related to that of the 1D-dictionary problem. The 1D-dictionary problem asks us to maintain a dynamic set of keys such that we can answer membership queries, i.e. whether or not a key is contained in the set or not. We can reduce the three-sided range reporting problem to the 1D-dictionary by restricting elements to be of the form $(x,x)$ and test membership by a query of the form $\left[x,x\right] \times \left[-\infty, \infty\right]$. 
By reduction we must have that the lower bounds of the 1D-dictionary problem also applies to that of the three-sided range queries.

The 1D-dictionary problem has been a popular topic in the case of internal memory. It has been proved by a simply adversary argument that a query can be forced to cost $\log_2 N$ comparisons no matter the cost of updates, and more generally it has been proved that if an insertion performs at most $\mathcal{O}(k)$ comparisons then queries can be forced to cost at least $\max\left\lbrace \log_2 N, N/2^{\Theta(k)}\right\rbrace$ comparisons~\cite{Borodin81efficientsearching}.

There has also been much work on the lower bounds of the 1D-dictionary problem in external memory. We here give an adversary argument which shows that for any comparison based dictionary storing $N$ elements, there exists a query requiring at least $\log_B \frac{N}{M} - \mathcal{O}(1)$ I/O's, i.e. a lower bound for queries in external memory dictionaries. The argument goes like this: 

Assume we are at a position in our dictionary where the elements that can still be equal to our query are denoted \textit{candidate elements}. These elements form a consecutive subsequence in the partial ordering of the $N$ elements in the dictionary.
Initially we can have at most $M$ elements in internal memory. The adversary will now select a partial ordering of these $M$ elements, i.e. select answers to each comparison between these $M$ elements, such that there are at least $\frac{N-M}{M+1} > \frac{N}{M+1}-1$ candidate elements left.
Each I/O will bring in $B$ elements. If we have $k$ candidate elements before this I/O then the adversary will choose a partial ordering such that there are at least $\frac{k-B}{B+1} > \frac{k}{B+1}-1$ candidate elements left.
An argument by induction will show that after $i$ I/O's there are at least $\frac{N}{(M+1)(B+1)^i} - 2$ candidate elements left. As a consistent answer to the membership query cannot be given before we have only one candidate element left we must have that $\frac{N}{(M+1)(B+1)^i} - 2 \leq 1 \Rightarrow i = \log_{B+1} \frac{N}{M} - \mathcal{O}(1)$.

As mentioned in Section~\ref{sec:prelim_b_tree}, the B-Tree is the external memory version of a binary search tree. The query bounds of the B-Tree, $\mathcal{O}(\log_B N + K/B)$, are optimal, i.e. equal to the lower bound, but this is not the case for the update bounds.

\clearpage

Brodal and Fagerberg~\cite{Brodal:2003:LBE:644108.644201} studied two lower bound trade-offs between the I/O complexity of membership queries and updates. They arrive at the following theorem:
\begin{theorem}
If $N$ insertions perform at most $\delta \cdot N/B$ I/O's, for $1 \leq \delta \leq B \log_B N$ then
\begin{enumerate}
	\item There exists a query requiring at least $\log_{B+1} \frac{N}{M} - \mathcal{O}(1)$ I/O's.
	\item There exists a query requiring $N/(M\cdot (\frac{M}{B})^{\mathcal{O}(\delta)})$ I/O's for $N > M$.
	\item There exists a query requiring $\Omega(\log_{\delta \log^2 N} \frac{N}{M})$ I/O's for N > M.
\end{enumerate}
\end{theorem}

The first is essentially just the result proved by the above adversary argument saying that B-Trees have an optimal query bound. This result is summarized in Figure~\ref{fig:lower_bound_summary}.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../figures/lower_bound_summary}
\caption{A summary of the results of Brodal and Fagerberg~\cite{Brodal:2003:LBE:644108.644201}. It depicts the trade-off between insert and search/query. On one end we can achieve really fast insert operations but pay the price with slow queries. In the other end we have the B-Tree which query bound matches the optimal but has suboptimal insertions. No results exists for the gaps.}
\label{fig:lower_bound_summary}
\end{figure}

The results by Brodal and Fagerberg assume a comparison based model and that keys are indivisible. Iacono and P\v{a}tra\c{s}cu~\cite{Iacono:2012:UHS:2095116.2095164} looks at what happens when we remove this assumption and are thus allowed to use hashing. They improve the update time of the Buffer Tree by roughly a logarithmic factor. More precisely they arrive at the following theorem:

\begin{theorem}
For any $\max\left\lbrace \log\log N, \log_M N \right\rbrace \leq \lambda \leq B$, we can solve the dictionary problem by a Las Vegas data structure with update time $t_u = \mathcal{O}(\frac{\lambda}{B})$ and query time $t_q = \mathcal{O}(\log_\lambda N)$ with high probability.
\end{theorem}

This means that in one end of the trade-off they can for $\lambda = B^\epsilon$ obtain an update time of $\mathcal{O}(1/B^{1-\epsilon})$ and query time of $\mathcal{O}(\log_B N)$. This matches the bounds of the Buffer Tree of Arge. 

If we instead set out to obtain fast updates, they are able to achieve an update bound very close to the optimal disk transfer rate of $1/B$ namely they obtain $t_u^{\min} = \mathcal{O}(\frac{1}{B} \cdot \max\left\lbrace \log\log N, \log_M N \right\rbrace)$ but at the cost of a query time of $t_q^{\min} = \mathcal{O}\left(\log_{\max\left\lbrace \log\log N, \log_M N \right\rbrace} N\right)$.

These results suggest that there are still many possibilities to improve results for external memory structures if we abandon the comparison and indivisibility paradigms.

\begin{savequote}[0.5\textwidth]
``All my best thoughts were stolen by the ancients.''
\qauthor{--- Ralph Waldo Emerson}
\end{savequote}
\chapter{Internal Memory Priority Search Tree}
\label{chp:internal_pst}
In this chapter we present a static internal memory data structure for the three-sided range reporting problem. The data structure was originally presented by McCreight \cite{DBLP:journals/siamcomp/McCreight85} and is denoted a Priority Search Tree. The Priority Search Tree can be constructed in linear time and is basically a combination of a binary search tree on the $x$-coordinate and a heap on the $y$-coordinate. A formal definition of a Priority Search Tree for a set of $N$ points, $P$, is as follows. We assume that all points have distinct coordinates, though this assumption can be removed by using the normal lexicographical ordering of points.

\begin{itemize}
	\item If $P = \varnothing$ then the Priority Search Tree is an empty leaf.
	\item Otherwise, let $p_\text{max}$ be the point in the set $P$ with the largest $y$-coordinate.
	
			Let $x_{\text{mid}}$ be the median of the $x$-coordinates of the remaining points.
			
			Now let
			$$ P_\text{below} \coloneqq \{p \in P \setminus \{p_\text{max} \} : p_x \leq x_\text{mid} \}$$
			$$ P_\text{above} \coloneqq \{p \in P \setminus \{p_\text{max} \} : p_x > x_\text{mid} \}$$
			
			The Priority Search Tree consists of a root node $v$ where the point \\
			$p(v) \coloneqq p_{\text{max}}$ and the value $x(v) \coloneqq x_{\text{mid}}$ are stored. Furthermore,
			\begin{itemize}[label=$\bullet$]
				\item the left subtree of $v$ is a Priority Search Tree for the set $P_{\text{below}}$
				\item the right subtree of $v$ is a Priority Search Tree for the set $P_{\text{above}}$
			\end{itemize}
\end{itemize}

What is important to note is that the specific construction method allows the data structure to be indexed in two different ways. First, the tree can be searched as a binary search tree based on the $x$-coordinate. Second, the tree operates as a max-heap based on the $y$-coordinate. Please refer to Figure~\ref{fig:static_pst} for an illustration of a Priority Search Tree constructed on points $P = \{A, B, C, D, E, F, G, H\}$.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.68]{../figures/internal_pst_static}
	\caption{The letter given in the node represents the $y_{\text{max}}$ stored as a key, while the dashed line represents the median $x$ value stored in the node.}
	\label{fig:static_pst}
\end{figure}

\section{Three-sided range query}
When answering a query of the form $[x_1,x_2] \times [y,\infty]$ we begin at the root of the tree and first check the $y$-coordinate of this node against the $y$-coordinate of the baseline of the query interval. As long as the $y$-coordinate of the current node is greater than the $y$-coordinate of the horizontal segment defining the base of our three-sided range, we will continue down the tree. We can use the fact that the tree is a basic binary search tree on the $x$-coordinates to only visit the part of the tree that is within our search range. Please refer to Figure~\ref{fig:static_pst_query} for an illustration of the general query pattern.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{../figures/internal_pst_query}
	\caption{The shaded subtrees in the figure stores only points with $x$-coordinate within the correct range. This property ensures we can search the subtrees based on $y$-coordinate only.}
	\label{fig:static_pst_query}
\end{figure}

\subsection*{Analysis}
The search operation follows two root to leaf paths each of length $\mathcal{O}(\log N)$. Using the search path and heap property of the tree we are guaranteed to only visit nodes that is actually reported when querying. This gives a total running time of $\mathcal{O}(\log N + K)$.

\section{Dynamic Priority Search Tree}
The key difference between the static solution presented by McCreight and a dynamic solution is that we always ensure that each point is placed in exactly one leaf and the order of the leafs from left to right corresponds to the order of the $x$-coordinate of the points. The internal nodes of the tree store the point with greatest $y$-coordinate represented by a leaf in the subtree of the node which is not already stored by an ancestor of this interior node. It is clear we have $N$ leaf nodes and $N-1$ internal nodes in the data structure.
Whenever we store a point in an interior node, then the leaf node which corresponds to this point is considered a \textit{place-holder} for this point. A total of $N - 1$ nodes are left as place-holders. Please refer to Figure~\ref{fig:dynamic_pst} for an illustration of a dynamic Priority Search Tree over the points $P = \{A, B, C, D, E, F, G \}$.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.68]{../figures/internal_pst_dynamic}
	\caption{8 data points and the corresponding dynamic Priority Search Tree. Dotted leafs are place-holders for a key higher up in the tree.}
	\label{fig:dynamic_pst}
\end{figure}

\section{Construction}
Assuming we are given a list of $x$-sorted points we can construct a balanced dynamic Priority Search Tree using a bottom-up construction method similar to the bottom-up construction of a heap. In the first phase of construction we associate each point with a place-holder in the Priority Search Tree. These place-holders will become the leaf level of the data structure. Next we select pairs of place-holders and compare them to one another in terms of their $y$-coordinate. We denote the point with the highest $y$-coordinate as the \textit{winning} point and the comparison between points as a \textit{tournament round}. It is the winning point that will be represented by a new internal node at one level higher in the tree. Please refer to Figure~\ref{fig:dynamic_pst_construction} for an illustration of the construction of the leaf level of the data structure.
At the next level of the tree, we perform the same comparisons as before to determine which nodes will advance to the third level. At most $\nicefrac{N}{2}$ points are compared at this level.

Every tournament round will potentially leave an empty interior node behind as the wining point is moved one level up. If this occurs we have to check if any previous tournament losers are now eligible to be represented higher in the tree. We must also remember that some interior nodes in the tree may not represent any points and will thus remain empty when the construction is complete.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.68]{../figures/internal_pst_dynamic_construction}
	\caption{The first phase of the bottom-up construction "tournament".}
	\label{fig:dynamic_pst_construction}
\end{figure}

\subsection*{Analysis}
First, all points are inserted as leafs in the bottom layer of the data structure. The leafs counts for $N/2$ of the total nodes in the tree. The next $N/2^2$ nodes are created at height $h = 1$ and the tournaments are played one level down. For the $i$'th step we create $N/2^i$ nodes and we play tournaments $i$ levels down. The total steps to build the dynamic Priority Search Tree of size $N$ is thus:

$$\sum\limits_{i=0}^{\log(N)} \frac{N}{2^{i+1}}i = \frac{N}{2} \left( \sum\limits_{i=0}^{\log(N)} i\left(\frac{1}{2}\right)^i \right) \leq \frac{N}{2} \left( \sum\limits_{i=0}^{\infty} i\left(\frac{1}{2}\right)^i \right)$$

The solution to the last summation can be found by taking the derivative of both sides of the well known geometric series:

$$ \frac{\partial}{\partial x} \left( \sum\limits_{i=0}^{\infty} x^i \right) = \frac{\partial}{\partial x} \left( \frac{1}{1-x} \right) \Rightarrow \sum_{i=1}^{\infty} ix^i = \frac{x}{(1-x)^2} $$

For $x = \frac{1}{2}$ we get

$$\frac{1/2}{(1-1/2)^2} = 2$$

Plugging this in to the above sum we get that the total number of steps to build a dynamic Priority Search Tree on $N$ points is $\mathcal{O}(N)$.

This implies we can construct the balanced dynamic Priority Search Tree in linear time assuming we are given a sorted input. The tree is balanced since we place all leafs at the same level. If the size of the input is not a perfect power of 2, the tree will be imbalanced by a single level.

\section{Insertion}
In order to dynamically insert points into the data structure, we add a new leaf (place-holder) for the new point and perform a push-down operation with the new point starting at the root as will be described below.

We can determine where to add the new place-holder as the dynamic Priority Search Tree is a binary search tree on the $x$-coordinate of the points. When we reach an existing leaf, we add a new internal node in place of this leaf, and make the existing leaf one of the children of this new internal node. Then we add a new leaf to the tree as the other child, and store the new point in this leaf. See Figure~\ref{fig:dynamic_pst_insert_step1}.

\begin{figure}[b]
	\centering
		\includegraphics[width=0.7\textwidth]{../figures/internal_pst_insert_step1}
	\caption{First step of the insertion algorithm. A new place-holder is created for the new point to be inserted and a new internal node is created as its parent. Here point $B$ is inserted. Note that the ordering of the $x$-coordinates of the points is preserved in the order of the leafs following the addition of a new place-holder.}
	\label{fig:dynamic_pst_insert_step1}
\end{figure}


In order to maintain the heap order of the Priority Search Tree we now perform a push-down operation, where we at each step compare the $y$-coordinate of the point to be inserted with the $y$-coordinate of the point represented by the given internal node. If the $y$-coordinate of the point to be inserted is less than that of the point stored in the internal node, then we push the point to be inserted further down the tree. However, if the $y$-coordinate of the point to be inserted is greater than the $y$-coordinate of the point in the internal node, then we store the point to be inserted in this internal node, and take the point which was formerly represented by this internal node and continue the push-down operation with this point instead. See Figure~\ref{fig:dynamic_pst_insert_step2}.

\begin{figure}[H]
	\centering
		\includegraphics[width=1.0\textwidth]{../figures/internal_pst_insert_step2}
	\caption{Second step of the insertion algorithm. The push-down operation of point B starts at the root. Here point $B$ has smaller $y$-coordinate than points $F$ and $E$. Point $B$ has larger $y$-coordinate than point $C$ and so it occupies the internal node and pushes point $C$ further down the tree.}
	\label{fig:dynamic_pst_insert_step2}
\end{figure}

\subsection*{Analysis}

The first step of the insertion algorithm is a binary search traversal on the $x$-coordinate of the new point. The path is of length $\mathcal{O}(\log N)$. Adding a new internal node in place of the old leaf takes a constant amount of operations. The push-down operation follows a single root to leaf path of length $\mathcal{O}(\log N)$ and uses a constant amount of work in each node. We conclude insertion of an element can be done in $\mathcal{O}(\log N)$.

\section{Deletion}

When dynamically deleting a point we must locate the interior node (if any) representing the point we wish to delete. After we have removed the point from the interior of the dynamic search tree, we must replay a portion of the tournament among the points below this interior node in order to replace it. Finally we must delete the leaf which is the place-holder for the point.

\subsection*{Analysis}
Locating the interior node representing the point to be deleted can be done in $\mathcal{O}(\log N)$ using the binary search property maintained by the Priority Search Tree. Once we have located the interior node we can remove this point in $\mathcal{O}(1)$. The deletion of the point of an interior node leaves a hole in the tree that we fill by replaying tournaments following a node to leaf path of length $\mathcal{O}(\log N)$. We conclude the deletion algorithm requires $\mathcal{O}(\log N)$ per deletion.


\section{Rebalancing}
If we use the operations for dynamic updates as stated above without any measures for tree rebalancing, we could end up with a highly unbalanced tree. We fix this using global rebuilding when a linear number of updates have been performed. We can collect all points in sorted order in linear time by visiting leafs from left to right using a depth first traversal of the tree. On the collected points we now use the linear construction algorithm to rebalance the tree. Using this strategy yields a data structure that handles updates in $\mathcal{O}(\log N)$ amortized. By using a Red-Black tree as the heart of the tree we can achieve a data structure that is $\mathcal{O}(\log N)$ worst case by performing rotations to rebalance the tree.

\section{Bounds in the I/O model}
The above bounds translate directly to the I/O model as we cannot guarantee that nodes on the search path are perfectly placed in blocks, which in the worst case means that each visit to a node will equal 1 I/O.

\begin{savequote}[0.5\textwidth]
``Truth, like gold, is to be obtained not by its growth, but by washing away from it all that is not gold.''
\qauthor{--- Leo Tolstoy}
\end{savequote}
\chapter{External Memory Priority Search Tree}
\label{chp:arge_pst}
In this chapter we present an older result on dynamic three-sided range queries due to Arge et al.~\cite{arge_samoladas_vitter_1999}.
The result is a weight-balanced B-Tree where each node is augmented with a bootstrapped structure for storing the top $\Theta(B^2)$ points w.r.t. the $y$-value of the subtree rooted at that node. The bootstrapped structure is described in Section~\ref{sec:child_structure} and the main data structure of Arge et al.~that proves Theorem~\ref{thm:arge_structure} is described in Section~\ref{sec:arge_structure}.

\begin{theorem}
\label{thm:arge_structure}
An external memory data structure exists supporting insertion and deletion in amortized $\mathcal{O}(\log_B N/B)$ I/O's and reporting of three sided range queries in $\mathcal{O}(\log_B N/B~+~K/B)$ I/O's, where $N$ is the input size, and $K$ is the size of the output. The structure uses $\mathcal{O}(N/B)$ space.
\end{theorem}

\section{Dynamic 3-sided queries on \texorpdfstring{$\Theta(B^2)$}{tb2} points}
\label{sec:child_structure}
In this section we describe a data structure that supports the operations stated in Theorem~\ref{thm:child_structure}.
\begin{theorem}
\label{thm:child_structure}
There exists a dynamic data structure for storing $\mathcal{O}(B^{1+\epsilon})$ two dimensional points for $0 \leq \epsilon \leq 1$.
Insertion and deletion of $s$ points requires amortized $\mathcal{O}(1+\nicefrac{s}{B^{1-\epsilon}})$ I/O's.
The data structure supports reporting all points inside a query range of the form $[x_1,x_2] \times [y,\infty]$ in $\mathcal{O}(1+\nicefrac{K}{B})$ I/O's.
The structure uses linear space.
Finally the structure can be constructed using $\mathcal{O}(B^{1+\epsilon} / B)$ I/O's given a x-sorted set of $B^{1+\epsilon}$ points.
\end{theorem}

The structure consists of a static structure $\mathcal{L}$ storing $\mathcal{O}(B^{1+\epsilon})$ points and two buffers $\mathcal{I}$ and $\mathcal{D}$ storing at most $B$ points each. The buffers $\mathcal{I}$ and $\mathcal{D}$ store delayed insertions and deletions, respectively, and are initially empty. A point can appear in either $\mathcal{I}$ or $\mathcal{D}$ but not both as updates from either cancel each other out.

Let $L$ be the points stored in $\mathcal{L}$ and let $\ell = \lceil \nicefrac{\vert L \vert}{B}\rceil$. When $\mathcal{L}$ is fully constructed it will consists of $2\ell-1$ blocks of $B$ points in each block. The points in $L$ are first partitioned into blocks $b_1,\dots,b_\ell$ sorted by $x$-value. The last block may have size less than $B$.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1]{../figures/sweep-line}
	\caption{The structure for $B=4$. The points are represented by circles. The sweep line has merged blocks $b_1$ and $b_2$ at the point where the blocks contain $4$ points on or above the line. This is represented by a line segment with black endpoints and the $b_{1,2}$ label. The same goes for the other merged blocks created.}
	\label{fig:sweep-line}
\end{figure}

\textbf{To construct} blocks $b_{\ell+1},\dots,b_{2\ell}$ we make a vertical sweep over the points in increasing $y$-order. When the sweep line reaches a point in a block $b_i$ that together with an adjacent block, i.e. either $b_{i-1}$ or $b_{i+1}$, contains exactly $B$ points on or above the sweep line, we replace the two blocks by a single block containing the $B$ points on or above the sweep line.  The merged block is denoted $b_{i,j}$ if it contains points from the initial blocks in the range from, and including, $i$ to $j$. The two merged blocks are then excluded from the sweep and the newly created merged block is included in the continued sweep. Every merge of adjacent blocks causes the sweep line to intersect one block less resulting in at most $\ell-1$ blocks being created from the sweep.

A catalogue structure stores in $\mathcal{O}(1)$ disk blocks a reference to each of the $2\ell-1$ blocks. For block $b_i$ we store the minimum and maximum $x$-values for the points contained in the block. For a merged block $b_{i,j}$ we store the interval $\left[ i,j\right]$ and the minimum $y$-value of the points in the block. Note, this minimum $y$-value is also the point where the sweep line created the block $b_{i,j}$.

%%%%%%%%%%%%%
% UPDATES %%%
%%%%%%%%%%%%%
\textbf{Insertions and deletions} are stored in $\mathcal{I}$ and $\mathcal{D}$ respectively. When a point is inserted in $\mathcal{I}$ or $\mathcal{D}$ we make sure to remove any existing occurrence of the point in $\mathcal{I}$ and $\mathcal{D}$ such that the new update overrides any previous updates. Whenever $\mathcal{I}$ or $\mathcal{D}$ overflows, i.e. $\vert \mathcal{I} \vert > B$ or $\vert \mathcal{D} \vert > B$, the stored updates are applied to the set of points in $\mathcal{L}$. This is done by scanning $L$ in increasing $x$-order while applying insertions and deletions, i.e. for each point in $L$ we check whether we should insert a new point from $\mathcal{I}$ before it or if the point should be deleted. This process results in a new set of points $L'$ which once again is partitioned into blocks $b_1,\dots,b_{\ell'}$ and a vertical sweep similar to the previously described sweep is performed to rebuild the merged blocks and catalogue structure.
This reconstruction is done in $\mathcal{O}(\ell')$ I/O's. As $\ell' \leq \lceil \nicefrac{(\vert L \vert + 1)}{B}\rceil$ it requires $\mathcal{O}(\lceil \nicefrac{\vert L \vert}{B}\rceil) = \mathcal{O}(B^\epsilon)$ I/O's to rebuild $\mathcal{L}$. If we amortize this cost over the $>B$ updates that caused the overflow the cost becomes $\mathcal{O}\left(\nicefrac{B^\epsilon}{B}\right) = \mathcal{O}\left(\nicefrac{1}{B^{1-\epsilon}}\right)$ amortized I/O's per delayed update.

%%%%%%%%%%%%%
% QUERIES %%%
%%%%%%%%%%%%%
\textbf{Queries} are of the form $[x_1,x_2] \times [y,\infty]$ and can be answered by scanning the catalogue to find the blocks intersected by the sweep when it was at $y$. This corresponds directly to the $t$ line segments immediately below the line segment imposed by the bottom of the query range. These blocks will contain a superset of the points contained in our query.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1]{../figures/sweep-line-with-query}
	\caption{The grey area is our query and we should report the points within. This is done by finding the fat line segments which is the segments just below the sweep line at $y$. The segments can be found using the catalogue and the blocks can be scanned and relevant points can be reported in $\mathcal{O}(1+\nicefrac{K}{B})$.}
	\label{fig:sweep-line-query}
\end{figure}

We know from construction that the blocks intersected contains $B$ points on or above the sweep line. The left most and right most of these blocks are not necessarily fully contained in the query range and do not necessarily contain any points to report. We know that blocks must contain at least $B\lfloor \nicefrac{(t-2)}{2}\rfloor$ points since two adjacent blocks in the query range at the sweep line would otherwise have been merged to a single block containing just $B$ points, i.e. if we force merge all adjacent blocks two and two we would end up with $\nicefrac{(t-2)}{2}$ blocks each with at least $B$ points on or above the sweep line. It follows that the output is at least $K \geq B\lfloor \nicefrac{(t-2)}{2}\rfloor$.

The $t$ relevant blocks are scanned and the points contained in the query are reported. The total number of I/O's required becomes $\mathcal{O}(1+t) = \mathcal{O}(1+\nicefrac{K}{B})$ as $t \leq 2\frac{K}{B}-2$ from the previous observation.

We have now showed that we are able to construct a dynamic data structure with the bounds stated in Theorem~\ref{thm:child_structure}.

\hfill$\blacksquare$

\section{Main structure}
\label{sec:arge_structure}
As mentioned earlier, the main structure is a weight-balanced B-Tree~\cite{arge_vitter_1996} on the normal lexicographical ordering of points with regards to $x$. Each internal node of the structure stores an instance of the bootstrapped structure described above for answering three-sided queries on $\Theta(B^2)$ points. Arge et al. denotes this structure the \textit{query data structure} as it allows for fast queries which will be explained later. Points are stored in the query data structure according to the following rules.
\begin{itemize}
	\item{An internal node stores at most $B^2$ points in the associated query data structure.}
	\item{For a child $w$ of internal node $v$ the Y-set of $w$ denoted $Y(w)$ is the points of the query data structure of $v$ that is associated with the range that is associated with $w$. See Figure~\ref{fig:arge_child_strucutre}.}
	\item{An internal node stores at most $B$ points for each child of the node, i.e. for all children $w$ of an internal node $v$ we have that the size of $Y(w)$ is at most $B$.}
	\item{A leaf stores at most $2k$ points in its query data structure where $k$ is the leaf parameter of the B-Tree.}
	\item{If a node or leaf $v$ stores points in its query data structure then $Y(v)$ in $parent(v)$ must contain at least $B/2$ points.}
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{../figures/arge_child_structure}
	\caption{An internal node $v$ of the base tree. For each child $w$ of $v$, the Y-set $Y(w)$ consists of the $\Theta(B)$ highest points stored in the subtree of $v$ that are within the $x$-range of $w$. The Y-sets of the five children of $v$ are indicated by bold points. They are stored collectively in the query structure of $v$.}
	\label{fig:arge_child_strucutre}
\end{figure}


The base B-Tree uses linear space and since each point is stored only once in a query data structure that uses linear space, we can conclude that the structure stores $N$ points in $\mathcal{O}(N/B)$ blocks, i.e. uses linear space.

\clearpage

\section{Updates}
\label{sec:arge_updates}
\subsection{Insertion}

\begin{figure}[b]
	\centering
     \includegraphics[width=\textwidth]{../figures/arge_split}
     \caption{$v$ is split into $v'$ and $v''$. As a result $Y(v')$ and $Y(v'')$ may contain fewer than $B/2$ points.}
     \label{fig:arge_split_1}
\end{figure}

Inserting a point in the structure involves two steps. The first is to insert the point in the base B-Tree. This is done as described in Section~\ref{sec:prelim_b_tree} and may result in nodes splitting which in turn might result in the splitting of query data structures. Let $v$ be a node in the tree that has just been split into $v'$ and $v''$ as depicted in Figure~\ref{fig:arge_split_1}.

As a result $Y(v')$ and $Y(v'')$ may contain fewer than $B/2$ points. This is remedied by promoting points of $v'$ (resp. $v''$) into $Y(v')$ (resp. $Y(v'')$). Promoting a point from $v'$ to $parent(v')$ is done by finding the top-most point $p'$ stored in the query data structure of $v'$ in $\mathcal{O}(1)$ I/O's using the block structure of $Q_{v'}$. The points found are as shown in Figure~\ref{fig:arge_split_2}. Now, $p'$ is deleted from $Q_{v'}$ and inserted into $Q_{parent(v')}$. This process might cause one of the $Y$ sets of the children to become too small and we thus need to recursively promote a point. This recursion might in the worst case be on a path from $v$ down to a leaf. The process is called \textit{bubble-up}.

\begin{wrapfigure}{O}{0.6\textwidth}
\captionsetup{width=0.55\textwidth}
	\centering
		\includegraphics[width=0.55\textwidth]{../figures/arge_split_2}
	\caption{Too small $Y$-sets are remedied by promoting the topmost points from the children.}
	\label{fig:arge_split_2}
\end{wrapfigure}


After inserting in the base tree and appropriate reorganization we need to insert the point in the correct query data structure. The search starts in the root. The child $w$ responsible for the $x$-range which the points belongs to is found and its Y-set is found by a degenerate query on the form  $\left[ x\text{-range of }w \right] \times \left[ - \infty, \infty \right]$ to the query data structure of the root. If the number of points is $\geq B/2$ and the point is below all of them the point is recursively inserted into the found child. Otherwise the point joins the query data structure of the root. If the $Y$-set of the found child is now too large we recursively insert the lowest of these points into the child's query data structure. If we reach a leaf we simply insert the point into the query data structure of the leaf. See Figure~\ref{fig:arge_bubble_down}.

\begin{figure}[t]
	\centering
	\includegraphics[width=1\textwidth]{../figures/arge_bubble_down}
	\caption{Bubble down. First a degenerate query on the form $\left[ x\text{-range of }w \right] \times \left[ - \infty, \infty \right]$ is done for the found child $w$ resulting in a set of points with size $K$ (a) If $K < B/2$ then we simply insert the point (here marked as a circle) (b) If $K \geq B/2$ and the point is \textit{larger} than the \textit{smallest} $y$-value then the point is inserted and the smallest $y$-value is recursively sent down (c) If $K \geq B/2$ and the point is \textit{smaller} than the \textit{smallest} $y$-value then the point itself is sent recursively down.}
	\label{fig:arge_bubble_down}
\end{figure}

\subsection{Deletion}
Rebalancing the tree after handling a delete is done by adopting global rebuilding instead of using fusion of nodes. To delete a point we search down in the base tree for the point and mark it as deleted without actually removing the point. The next step is to remove the point from the query data structure that it resides in. This is done similar to finding the query data structure to insert the point into. The $Y$-set is recursively found on the search path of the point and if the $Y$-set contains the point, then the point is removed. If the $Y$-set becomes too small as a result we perform a \textit{bubble-up} operation.

\subsection{Analysis}
Inserting in the base tree can be done in $\mathcal{O}(\log_B N)$ I/O's by Section~\ref{sec:prelim_b_tree} and can cause as many splits on the path from a leaf to the root. Each split might cause $B/2$ \textit{bubble-up} operations. Each \textit{bubble-up} at $v$ costs $\mathcal{O}(1)$ I/O's and might recurse all the way to a leaf for a total of $\mathcal{O}(\log_B weight(v))$ where $weight(v)$ is the size of the subtree rooted at $v$. In the worst case $B/2$ of these operations are performed totalling at $\mathcal{O}(B\log_B weight(v)) = \mathcal{O}(weight(v))$ I/O's.

It follows from Lemma~\ref{lma:weight_balanced} that the cost of splitting a node can be amortized over the insertions and thus each of the $\mathcal{O}(\log_B N)$ splits cost $\mathcal{O}(1)$ I/O's amortized.

Deleting can be done in $\mathcal{O}(\log_B N)$ I/O's as it is just a search for the point in the base tree and query data structure.

Rebalancing of the tree is done using global rebuilding. After $\Theta(N)$ delete operations the tree is rebuilt using $\mathcal{O}(N \log_B N)$ I/O's which is paid for by double charging the $\Theta(N)$ delete operations.
\section{Query}
\label{sec:arge_query}
Querying the data structure with $Q = \left[ x_1, x_2 \right] \times \left[ y, \infty \right]$ consists of two steps. The first step is to identify which nodes to visit and the second consists of reporting points in $Q$ from the query data structures of the identified nodes. 
We identify which nodes to visit by searching on a path from the root to leaf along paths corresponding to $x_1$ and $x_2$ and by visiting nodes in between the two paths. As the tree is a search tree on the $x$-coordinate we know that nodes in between the search paths for $x_1$ and $x_2$ will be in the query range of $Q$.
We will only proceed to visit a child of $v$ if we report all points from the query data structure of $v$ in the $Y$-set of that child, with the exception of the leftmost and rightmost paths which are always visited all the way to a leaf.
We report all points in $Q$, since, by the rules, a point in $Q$ cannot be in an unvisited subtree as this would have been visited if all points were reported and no points in the subtree has lower $y$-values.

\subsection{Analysis}
In every internal node $v$ visited we spend $\mathcal{O}(1+K_v/B)$ I/O's. There are $\mathcal{O}(\log_B N)$ nodes on the search path from root to the leftmost leaf and rightmost leaf and thus the number of I/O's used on these paths is $\mathcal{O}(\log_B N + K/B)$. All other internal nodes visited are visited because all points were reported in the parent. If we do not report all points from a $Y$-set we can charge the $\mathcal{O}(1)$ I/O's of visiting the child to the parent which must have reported $\Theta(B)$ points. As the cost of reporting from the query data structures is $\mathcal{O}(1+K/B)$ the total cost amounts to $\mathcal{O}(\log_B N + K/B)$.


\begin{savequote}[0.50\textwidth]
``Daring ideas are like chessmen moved forward: they may be beaten, but they may start a winning game.''
\qauthor{--- Johann Wolfgang von Goethe}
\end{savequote}
\chapter{External Memory Buffered Priority Search Tree}
\label{chp:epst}
In this chapter we present an external memory data structure described by Brodal~\cite{DBLP:journals/corr/Brodal15}. The structure supports updates in amortized $\mathcal{O}\left(\frac{1}{\epsilon B^{1-\epsilon}} \log_B N\right)$ I/O's, three sided range queries in $\mathcal{O}\left(\frac{1}{\epsilon}\log_B N + \nicefrac{K}{B}\right)$ I/O's for $0 < \epsilon \leq 1$, and can be constructed on $N$ sorted points in $\mathcal{O}(\nicefrac{N}{B})$ I/O's. The parameter $\epsilon$ determines the size of the fanout and in turn the size of a bootstrapped substructure for storing $\mathcal{O}(B^{1+\epsilon})$ points.
The substructure is very similar to that of Arge et al.~\cite[Section~3.1]{arge_vitter_2003} for handling $\Theta(B^2)$ points with the main difference being that we reduce the capacity to allow an amortized constant number of I/O's per update. The bootstrapped structure is in~\cite{arge_vitter_2003} used to store the top $\Theta(B^2)$ points w.r.t. the $y$ value for the subtree rooted at the given node of the substructure. This structure uses it in a slightly different way to store the top $\mathcal{O}(B^{1+\epsilon})$ points of the children of the given  node. The bootstrapped structure is described further in Section~\ref{sec:child_structure} and will be referred to as the \textit{child structure} in the rest of the chapter.

The External Memory Buffered Priority Search Tree is a combination of the External Memory Priority Search Tree of Arge et al.~\cite{arge_samoladas_vitter_1999} described in Chapter~\ref{chp:arge_pst}, and the buffered updates of the Buffer Tree also thanks to Arge~\cite{Arge:1995:BTN:645930.672850} described in Section~\ref{sec:prelim_buffer_tree}. The main data structure is described in Section~\ref{sec:main_data_structure}.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../figures/gerth_tree}
	\caption{External Memory Priority Search Tree with buffers. The points stored in the point buffers $P_{c_i}$ of the children are also stored in the child structure $C_v$ of the parent. This allows for fast queries.}
	\label{fig:gerth_tree}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%
% Main data structure %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Main data structure}
\label{sec:main_data_structure}
This section presents the main data structure achieving the results of Theorem~\ref{thm:main_structure}.
\begin{theorem}
\label{thm:main_structure}
An external memory data structure exists supporting insertion and deletion of points in amortized $\mathcal{O}(\frac{1}{\epsilon B^{1-\epsilon}} \log_B N)$ I/O's and three sided range queries in amortized $\mathcal{O}(\frac{1}{\epsilon} \log_B N + \nicefrac{K}{B})$, where $\epsilon$ is a constant, $0 < \epsilon \leq 1$, $N$ is the number of points in the structure, and $K$ is the size of the output. The structure can be constructed in amortized $\mathcal{O}(\nicefrac{N}{B})$ I/O's on an x-sorted set of points and stored in $\mathcal{O}(\nicefrac{N}{B})$ blocks.
\end{theorem}

The structure is a slightly modified version of the B-Tree over the $x$-values of points. Each internal node, except for the root, has a node degree between $\Delta/2$ and $\Delta$, with $\Delta = \lceil B^\epsilon \rceil$. The root has node degree between $2$ and $\Delta$.

Each node $v$ stores three buffers containing $\mathcal{O}(B)$ points each, namely a point buffer $P_v$, an insertion buffer $I_v$, and a deletion buffer $D_v$ which purpose will be described shortly.

As in the Internal Memory Priority Search Tree of McCreight~\cite{DBLP:journals/siamcomp/McCreight85} described in Chapter~\ref{chp:internal_pst}, the points with highest $y$-value resides in the top of the tree, i.e. we have a heap ordering among the nodes of the tree on the $y$-value. This means that for a child $c$ of $v$, there is no elements in the point buffer $P_c$ of the child with larger $y$-value than the minimum $y$-value in the point buffer $P_v$ of $v$.

The buffers $I_v$ and $D_v$ stores delayed insertions and deletions on their way down to a point buffer of a descendant. Using the basic ideas of the Buffer Tree of Arge~\cite{Arge:1995:BTN:645930.672850} described in Section~\ref{sec:prelim_buffer_tree}, buffers are handled recursively whenever an invariant is broken. We will introduce the invariants in Section~\ref{subsec:brodal_invariants}.

For each internal node $v$ we also store an instance of the child structure $C_v$ containing a copy of all points stored in the point buffers $P_c$ of every child $c$ of $v$. See Figure~\ref{fig:gerth_tree}.

Finally, for each internal node, $v$, we store, in $\mathcal{O}(1)$ blocks, information about the minimum $y$-value of the points of each of $v$'s children. If a child is empty we will mark this by storing $\infty$ as the minimum $y$-value.

All information at the root is kept in internal memory except for the child structure.

\subsection{Invariants}
\label{subsec:brodal_invariants}
For a node $v$ in the main data structure the following invariants must be true:
\begin{itemize}
	\item $P_v$, $I_v$, and $D_v$ are disjoint and points in the buffers have $x$-values spanned by the subtree at $v$.
	\item All points in $I_v \cup D_v$ have $y$-value less than the points in $P_v$.
	\item An update in a buffer at $v$ is more recent than updates in descendants of $v$ with equal points, and thus, should overwrite any updates in descendant of $v$ with equal points.
	\item A leaf in the tree has empty insertion and deletion buffers and the size of its point buffer is less than $B/2$.
	\item An internal node in the tree has $B/2 \leq \vert P_v \vert \leq B$, $\vert D_v \vert \leq B/4$, and $\vert I_v \vert \leq B$.
\end{itemize}

\subsection{Updates}
\label{subsec:gerth_updates}
We update the structure with insertions and deletions by adding points to the root's insertion or deletion buffer respectively, while maintaining the above invariants.
During an update the insertion or deletion buffers might overflow, i.e get size larger than $B$ or $B/4$ respectively. This is handled in the following five steps:
\begin{inlinelist}
	\item handle overflowing deletion buffers
	\item handle overflowing insertion buffers
	\item split leafs with overflowing point buffers
	\item split nodes of degree $\Delta+1$
	\item fill underflowing point buffers.
\end{inlinelist}

We will in the following look at each step individually and argue their complexity.

\begin{enumerate}[label=(\roman*)]
	\item\label{update:del} A deletion buffer at node $v$ overflows when $\vert D_v \vert > B/4$. Since the structure is a B-Tree on the lexicographically ordering of points, by the pigeon-hole principle, there must exist a child $c$ such that we can push $U \subseteq D_v$ of $\lceil \vert D_v \vert / \Delta \rceil$ deletions to $c$. This is illustrated in Figure~\ref{fig:pigeon_hole}. Points in $U$ are removed from $D_v$, $I_c$, $D_c$, $P_c$, and $C_v$. Any point $p$ in $U$ lexicographically larger than the minimum point in $P_c$ (w.r.t. $y$) is removed from $U$ as the deletion cannot cancel any updates further down in the tree. See Figure~\ref{fig:brodal_deletion_buffer_overflow}.
	
	\begin{figure}[htp!]
		\centering
		\includegraphics[width=0.9\textwidth]{../figures/pigeon_hole2}
		\caption{Pigeon hole principle. Each cross represents a deletion stored in the deletion buffer of $v$. Here $B' = \nicefrac{B}{4} = 16$ and $\epsilon = \nicefrac{1}{3}$. This gives $B^{\epsilon} = 4$. By the pigeon hole principle we must have that at least one of the x-ranges contains at least $\nicefrac{B'}{B^{\epsilon}}$ points that can be sent down to the child that is responsible. Here the child $c_3$ will receive a subset of the deletions.}
		\label{fig:pigeon_hole}
	\end{figure}
	
\clearpage	

	\begin{figure}[t]
		\centering
		\includegraphics[width=0.77\textwidth]{../figures/brodal_deletion_buffer_overflow}
		\caption{Deletion buffer overflow. A total of $\lceil \lvert D_v \rvert /B^\epsilon \rceil$ deletions are moved from $D_v$ to $\mathcal{U}$. Deletions larger than the smallest $y$-value in $P_c$ are removed from $\mathcal{U}$ since they cannot cancel points further down because of the heap order of the tree. Finally all points in $\mathcal{U}$ are removed from $P_c$, $I_c$, $D_c$, and $C_v$ before $\mathcal{U}$ is inserted into $D_c$.}
		\label{fig:brodal_deletion_buffer_overflow}
	\end{figure}
	
	If $v$ is a leaf we do not need to do more. If not, the remaining points in $U$ are inserted in $D_c$ which might recursively overflow. In the worst case we might recursively overflow along a path from the root to a leaf each time causing $\mathcal{O}(\lceil B / \Delta \rceil)$ deletes to be pushed one level down. Updating $C_v$ with $\mathcal{O}(\lceil B / \Delta \rceil)$ updates takes amortized $\mathcal{O}(1+ (B/\Delta) / B^{1-\epsilon}) = \mathcal{O}(1)$ I/O's.
	
	\item\label{update:ins} An insertion buffer at $v$ overflows when $\vert I_v \vert > B$. Similar to handling a deletion buffer overflow we find a child $c$ such that we can push $U \subseteq I_v$ of $\lceil \vert D_v \vert / \Delta \rceil$ insertions to $c$. Points in $U$ are removed from $I_v$, $I_c$, $D_c$, $P_c$, and $C_v$.
	Any point $p$ in $U$ lexicographically larger than the minimum point in $P_c$ (w.r.t. $y$) is removed from $U$ and inserted into $P_c$ and $C_v$.
	If $P_c$ overflows, the lexicographically smallest points w.r.t. $y$ are moved from $P_c$ to $U$ and removed from $C_v$ until $P_c$ no longer overflows.
	If $c$ is a leaf then all points are inserted into $P_c$ and $U$ is now empty.
	Otherwise, the remaining points in $U$ are added to $I_c$ which might overflow and cause a similar overflow along a path from the root to a leaf in the worst case as in the case of the deletion buffer overflow. See Figure~\ref{fig:brodal_insert_buffer_overflow}. The analysis and bounds follows directly from that of the deletion buffer overflow.
	
	\begin{figure}[b]
		\centering
		\includegraphics[width=0.77\textwidth]{../figures/brodal_insert_buffer_overflow}
		\caption{Insert buffer overflow. A total of $\lceil \lvert I_v \rvert /B^\epsilon \rceil$ points are moved from $I_v$ to $\mathcal{U}$. All points from $\mathcal{U}$ are removed from $D_c$ since they cancel the deletions. Points larger than the smallest $y$-value in $P_c$ are inserted into $P_c$ and points smaller than the smallest $y$-value in $P_c$ are inserted into $I_c$. This ensures the invariant that the tree is heap ordered. Finally the newly added points to $P_c$ are also inserted into $C_v$ to ensure that $C_v$ contains a copy of all points in $P_c$.}
		\label{fig:brodal_insert_buffer_overflow}
	\end{figure}		
	
	\item\label{update:pbo} A point buffer overflows at a leaf $v$ when $\vert P_v \vert > B/2$. If this is the case then we split the leaf into two nodes and evenly distribute the points in $P_v$ among the two new nodes $v'$ and $v''$ using $\mathcal{O}(1)$ I/O's. See Figure~\ref{fig:brodal_point_buffer_overflow}. The splitting of the node might cause the parent to get a degree of $\Delta+1$.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=1\textwidth]{../figures/brodal_point_buffer_overflow}
		\caption{Point buffer overflow. The point buffer $P_v$ is evenly distributed between to new nodes $v'$ and $v''$. After the split we have to ensure the parent is not node degree overflowed.}
		\label{fig:brodal_point_buffer_overflow}
	\end{figure}	
	
	\item\label{update:deg} An internal node $v$ with a degree larger than $\Delta$ is split into two new nodes $v'$ and $v''$. $I_v$, $D_v$, and $P_v$ are distributed among $v'$ and $v''$ according to the $x$-value. Finally the child structures of $v'$ and $v''$ are rebuilt from the children's point buffers. See Figure~\ref{fig:brodal_node_degree_overflow}. The split might cause the parent of $v$ to have a degree overflow and in the worst case we need to split along a path from a leaf to the root. The splitting of a single node costs $\mathcal{O}(\Delta)$ I/O's due to the reconstruction of the child structures.
	
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=1\textwidth]{../figures/brodal_node_degree_overflow}
		\caption{Node degree overflow. The buffers $I_v$, $D_v$, and $P_v$ are distributed to the new nodes $v'$ and $v''$. After the split we have to ensure the child structures contains a copy of all points from the point buffers of the children.}
		\label{fig:brodal_node_degree_overflow}
	\end{figure}		
	
	\item\label{update:pbu} A point buffer underflows at $v$ when $\vert P_v \vert < B/2$. In that case we try to \textit{pull up} the highest $B/2$ points from the children of $v$ into $P_v$. If $v$'s subtree does not store any points then we remove all points from $D_v$ and move points from $I_v$ to $P_v$ until $\vert P_v \vert = B$ or $I_v = \emptyset$.
	Otherwise we use $\mathcal{O}(\Delta)$ I/O's to identify the set $X$ of the top $B/2$ points from the children of $v$ and remove the identified points from the point buffers of the children and the child structure $C_v$ of $v$.

	If a point buffer of a child becomes empty before having identified all of the top $B/2$ points we have to recursively fill that child as the subtree might contain points with larger $y$-value than the remaining children of $v$. After this is done we can continue to grab points from the children.
	
	All points in $X \cap D_v$ are then removed from $X$. This might cause $\lvert X \rvert < B/2 - \lvert P_v \rvert$ resulting in a repeated run of the procedure to guarantee $X$ contains enough points to ensure $P_v$ is no longer underflowed.
	
	The remaining points of $X$ are inserted into $P_v$ and the child structure of the parent of $v$. Please refer to Figure~\ref{fig:brodal_pb_underflow} for an illustration of the main ideas of the pull up procedure.
	

	
	The points of $X$ now inserted into $P_v$ might have a smaller $y$ value than the points in $I_v$. We solve this problem by swapping the highest point in $I_v$ with the lowest point in $P_v$ while there exists a point in $I_v$ that is higher than a point in $P_v$, and make sure to maintain the child structure of the parent to reflect the changes made to the insert and point buffer.
	
	If the subtree of $v$ becomes empty as a result of pulling points up to $v$ we must remove all points of $D_v$ and move points from $I_v$ to $P_v$. This might cause $P_v$ to overflow which is handled recursively.
	
	Finally, after having pulled points from the children, we check if any of the children's point buffers underflows and should be refilled.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{../figures/brodal_point_buffer_underflow}
		\caption{Point buffer underflow. Here $B = 32$, $\epsilon = 2/5$ giving $B^\epsilon = 4$. The point buffer $P_v$ contains less than $B/2$ points, i.e. it is underflowed. The point buffers of the children are considered and the top $B/2$ points are added to $X$. Now deletions from $D_v$ cancels points in $X$ and the remaining of $X$ is inserted into $P_v$ and the child structure of the parent $C_p$. Finally the heap order is maintained by swapping points between $P_v$ and $I_v$, and by reflecting these changes in $C_p$.}
		\label{fig:brodal_pb_underflow}
	\end{figure}
	
\end{enumerate}

\clearpage

\subsubsection*{Analysis}
The tree stays balanced during insertions since we only increase in height whenever the root splits, which causes every path from root to leaf to increase by one. In the B-Tree we handle rebalancing using fusion of nodes. We do not apply this method here. Instead we apply global rebuilding when a linear number of updates have been performed. By~\ref{update:pbo} it follows that the total number of leafs created during $N$ insertions can be at most $\mathcal{O}(N/B)$ implying that at most $\mathcal{O}(\frac{N}{\Delta B})$ internal nodes can be created by splitting internal nodes. From this it follows that the tree has height $\mathcal{O}(\log_\Delta \frac{N}{B}) = \mathcal{O}(\frac{1}{\epsilon} \log_B N)$.

We can now argue that every update in~\ref{update:del} and~\ref{update:ins} requires amortized $\mathcal{O}(\frac{1}{\epsilon B^{1-\epsilon}} \log_B N)$ I/O's. As every $\Theta (B/\Delta)$ update require $\mathcal{O}(1)$ I/O's on every layer of the tree we get the correct amortized bound:
\begin{align*}
\mathcal{O}\bigg(\frac{1}{B/\Delta} \log_\Delta \frac{N}{B}\bigg) &= \frac{B^\epsilon}{B} \log_{B^\epsilon} \frac{N}{B} \\
&= \frac{B^\epsilon}{B} \frac{1}{\epsilon} \log_{B} N \\
&= \frac{1}{\epsilon B^{1-\epsilon}} \log_B N
\end{align*}

In~\ref{update:pbo}, we know that at most $\mathcal{O}(N/B)$ leafs are created each requiring $\mathcal{O}(1)$ I/O's giving amortized $\mathcal{O}(1/B)$ I/O's per update.

In~\ref{update:deg}, we know that at most $\mathcal{O}(\frac{N}{B\Delta})$ internal nodes are created. The creation of such a node costs $\mathcal{O}(\Delta)$ giving an amortized cost of $\mathcal{O}(1/B)$ I/O's per update.

In~\ref{update:pbu} each refilling might trigger a cascaded recursive refilling of one or more of the children. Every refilling takes $\mathcal{O}(\Delta)$ I/O's and moves $\Theta(B)$ points one level up through the tree's point buffers. Each point can at most move $\mathcal{O}(\log_\Delta \frac{N}{B})$ levels up, as this is the tree's height. This means that the total number of I/O's for the refillings during the course of $N$ operations is amortized $\mathcal{O}(\frac{1}{B/\Delta} \log_\Delta \frac{N}{B}) = \mathcal{O}(\frac{1}{\epsilon B^{1-\epsilon}} \log_B N)$ per point.

This argument ignores the fact that when pulling up points some points might swap positions from $I_v$ to $P_v$. This swap does not change the fact that the number of points we pull up remain the same and therefore it does not affect the amortized accounting.

Another fact that we ignore is what happens if we are not able to pull up $B/2$ points from the children. This is solved by a simple amortization argument. We double charge the operation responsible for pushing points to a child. This way we can ensure each node with non-empty point buffers always has saved an I/O for being emptied by a recursive pull up.

\subsection{Global rebuilding}
Since we do not fuse nodes with too low node degree we might end up with an unbalanced tree. We use global rebuilding as described in Section~\ref{sec:prelim_global_rebuilding} to guarantee the tree never gets \textit{too} unbalanced which would disprove our amortized bounds. This is done by partitioning updates into epochs. After a rebuild a new epoch begins and if the data structure at this points stores $\bar{N}$ points, then the next epoch will begin after $\bar{N}/2$ updates, i.e. a global rebuild will be performed.
Having a new epoch after every $\bar{N}/2$ updates ensures that the tree does not grow higher than $\mathcal{O}\left(\frac{1}{\epsilon}\log_B\frac{3\bar{N}}{2}\right) = \mathcal{O}\left(\frac{1}{\epsilon}\log_B N\right)$ as the size is $\frac{1}{2}\bar{N} \leq N \leq \frac{3}{2}\bar{N}$.

Global rebuilding works by constructing an empty structure and then reinserting all the points of the old structure that has not been deleted.

The points to reinsert are found by doing a top-down traversal of the tree while flushing insertion and deletion buffers to children. The points to reinsert are then found in the point buffers after flushing. This might cause buffers to temporarily overflow but we will allow this as the old structure will be deleted.

Once the set of points to reinsert have been found we simply insert the points on an initially empty tree.

\subsubsection*{Analysis}

Elements at level $i$ (leaf layer being level 0) can at most be flushed $i$ levels down.
The structure holds at most $\frac{3\bar{N}}{2B}$ nodes in total and at level $i$ the structure has at most $\frac{3\bar{N}}{2B} \frac{1}{\Delta^i}$ nodes. The cost of flushing all the buffers at level $i$ becomes:

$$ i \cdot \frac{3\bar{N}}{2B} \frac{1}{\Delta^i}$$

By summing over all layers of the tree we get the total cost of flushing all buffers to be:
\begin{align*}
\sum\limits_{i=0}^{\mathcal{O}(\log_\Delta \frac{3N}{2B})} i \cdot \frac{3\bar{N}}{2B} \frac{1}{\Delta^i} &< 
\sum\limits_{i=0}^{\infty} i \cdot \frac{3\bar{N}}{2B} \frac{1}{\Delta^i} \\
&= \frac{3\bar{N}}{2B} \sum\limits_{i=0}^{\infty} \frac{i}{\Delta^i} \\
&= \frac{3\bar{N}}{2B} \sum\limits_{i=0}^{\infty} \left(\frac{1}{\Delta}\right)^i \\
&= \frac{3\bar{N}}{2B} \frac{1}{1-\Delta} \\
&= \mathcal{O}(N/B)
\end{align*}

This gives an amortized cost of $\mathcal{O}(1/B)$ per update to flush all buffers.

The $\mathcal{O}(\bar{N})$ reinsertions into the new, initially empty, tree can be done in amortized $\mathcal{O}(\frac{\bar{N}}{\epsilon B^{1-\epsilon}}\log_B\bar{N})$ I/O's which is paid for by the $\bar{N}/2$ updates during the epoch.

\subsection{Three sided range queries}
\label{subsec:brodal_3_sided}
Reporting a three-sided query $Q = \left[ x_1,x_2 \right] \times \left[y, \infty \right]$ consists of three steps. Namely, identifying the nodes to visit, push down delayed insertions and deletions between the identified nodes, and finally reporting the points contained in $Q$.

We identify the nodes to visit in a breadth first manner.
Starting from the root we identify, from the query's $x$-range, the children that are relevant to the query and push all insertions and deletions belonging to those children in the root to the identified children. This is done without handling possible overflows. After having done this we know that the point buffers of the children do not change further and we can thus report all points in the query range from the child structure and the point buffer of the root. The children worth visiting are then added to the back of the breadth first search queue. We can decide whether a child is worth visiting without reading the node by comparing $y$ with the minimum $y$-value of that child's point buffer. This follows from the heap-order of point buffers. If the query-$y$ lies above the minimum $y$-point of a child, then by the heap-order invariant, we know that no relevant points are to be found in the subtree rooted at that child. The minimum $y$-value for every child is stored in the parent.
All nodes except for the root do not need to report from their point buffers as the parent of the node has already reported the relevant points from the child structure.

After all points have been reported we might have some buffers that have temporarily overflowed. This is now handled in a bottom up fashion using the update operations described in Subsection~\ref{subsec:gerth_updates}. We will handle a single subtree at a time and make sure that the entire subtree has no broken invariants, i.e. buffers that overflow or underflow or any nodes with a too high node degree. We do this by applying the update operations~\ref{update:del}-\ref{update:deg}\footnote{\begin{inlinelist}
	\item handle overflowing deletion buffers
	\item handle overflowing insertion buffers
	\item split leafs with overflowing point buffers
	\item split nodes of degree $\Delta+1$
\end{inlinelist}
} while we disallow any recursion leaving the subtree, i.e. no splits may recursively split nodes outside of the subtree. Only when the entire subtree has no overflows or underflows we can remedy a potential underflowed point buffer using update operation~\ref{update:pbu}\footnote{(v) fill underflowing point buffers}, and only then are we allowed to continue to the next subtree one level higher in the tree. See Figure~\ref{fig:gerth_fixup}. This way of handling the fixup procedure ensures we never get interleaving update operations interfering with each other.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../figures/gerth_fixup}
\caption{Fixup of the tree after having reported a query. All broken invariants of subtrees at level $i$ are handled before handling broken invariants at level $i + 1$. Here, the grey subtrees respects all invariants and we are about to handle the white subtree. The node degree overflow at level $i + 1$ is handled \textit{after} all subtrees at level $i$ are valid w.r.t the invariants. }
\label{fig:gerth_fixup}
\end{figure}

\subsubsection*{Analysis}

Assume we during a query visit $V$ nodes not on the search paths for $x_1$ or $x_2$ and $\mathcal{O}(\frac{1}{\epsilon}\log_B N)$ nodes on the search paths. We know that the $V$ nodes must have at least $VB/2$ points in their point buffers before updates are pushed down. The number of deletions we push down to visited nodes can at most be $(V + \mathcal{O}(\frac{1}{\epsilon}\log_B N))B/4$. It now follows that the number of points we report, $K$, must be at least the number of points in the point buffers before pushing down minus the deletions we push down: $VB/2 - (V + \mathcal{O}(\frac{1}{\epsilon}\log_B N))B/4 = VB/4 - \mathcal{O}(\frac{B}{\epsilon}\log_B N) = K$. By isolating $V$ it follows that $V = \mathcal{O}(\frac{1}{\epsilon}\log_B N + K/B)$.

The worst case bound now becomes the sum of visiting the $V$ nodes, the nodes on the search paths for $x_1$ and $x_2$, and the output: $\mathcal{O}(V+\frac{1}{\epsilon}\log_B N + K/B) = \mathcal{O}(\frac{1}{\epsilon}\log_B N + K/B)$.
On top of this comes the cost of pushing down update buffer elements and handling overflowing update buffers and overflowing point buffers.

This cost of pushing $\Omega(B/\Delta)$ points to a child is however already paid for by the update operations and is thus covered by the analysis of Subsection~\ref{subsec:gerth_updates}. It is only when we push down $\mathcal{O}(B/\Delta)$ updates to a child, with an amortized cost of $\mathcal{O}(1)$ that this cost is covered by the cost of visiting the child.

Handling the overflowing update buffers and underflowing point buffers are also paid for by the update operations described in Subsection~\ref{subsec:gerth_updates}.

This all adds up to a total amortized cost of $\mathcal{O}(\frac{1}{\epsilon} \log_B N + K/B)$ I/O's for a three-sided range query.

\subsection{Construction}
The structure can be initialized with a set of $N$ points using $\mathcal{O}(\text{Sort}(N))$ I/O's. If the points are sorted on the $x$-coordinate we only need $\mathcal{O}(\text{Scan}(N))$. For the remainder of this section we assume that the points are initially sorted with regards to the $x$-coordinate.

The first step of the construction is to construct a B-Tree over the $x$-values. We let each internal node have a degree of $\Delta/2$ and each leaf stores $B/2$ points, with the exception of the rightmost leaf which might contain less than $B/2$ points, and the rightmost internal nodes having a degree less than $\Delta/2$.

The point buffers of the internal nodes are now filled bottom up pulling the top $B/2$ highest $y$-value points up. If this results in a child having an underflowing point buffer we recursively fill that child before proceeding. In a second iteration we do the same but in a top-down fashion.

All insertion and deletion buffers are initially empty and the child structures are constructed from the point buffers of the children.

This construction algorithm could also have been used for global rebuilding giving a matching amortized cost.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../figures/gerth_construction}
	\caption{A B-Tree with each internal node having a degree of $\Delta/2$. Each leaf stores $B/2$ points with the possible exception of the rightmost. The bold points are the top $B/2$ points that is pulled up from the leafs into the layer above.}
	\label{fig:gerth_construction}
\end{figure}

\subsubsection*{Analysis}

We know level $i$ of the tree contains at most $\frac{N}{B\Delta^i}$ nodes. It follows that the number of points stored at or above level $i$ is $\mathcal{O}\left(\sum_{j=i}^\infty B\frac{N}{B\Delta^j}\right) = \mathcal{O}\left(\frac{N}{\Delta^i}\right)$. It must follow that we cannot move $B/2$ points to level $i$ from $i-1$ more than $\mathcal{O}\left(\frac{N}{\Delta^i} / \frac{B}{2}\right)$ times. We know that we can move $B/2$ points using $\mathcal{O}(\Delta)$ I/O's and thus the total number of I/O's to fill the point buffers becomes:
$$\mathcal{O}\left(\sum\limits_{i=1}^\infty \Delta \frac{N}{B\Delta^i}\right) = \mathcal{O}\left(\frac{N}{B}\sum\limits_{i=1}^\infty \Delta \frac{1}{\Delta^i}\right) = \mathcal{O}\left(\frac{N}{B}\sum\limits_{i=0}^\infty \frac{1}{\Delta^i}\right) = \mathcal{O}\left(\frac{N}{B}\right)$$

Another aspect we have to look at is what happens to the amortized analysis when we initialize our data structure using this construction method, i.e. we have to argue that the amortized cost of the remaining operations remain unchanged during the epoch started by the construction.

In order to argue this we consider a sequence of operations containing $N_{ins}$ insertions and $N_{del}$ deletions and a newly constructed tree of $N$ points.

Let us first consider the cost of creating new nodes in the tree. Each leaf has initially at most $B/2$ points and it follows that we can at most create $2N_{ins}/B$ new leafs. Each new leaf is created in $\mathcal{O}(1)$ I/O's and it thus cost at most $\mathcal{O}(N_{ins}/B)$ I/O's to create new leafs during $N_{ins}$ insertions.
By a similar argument it follows that at most $\mathcal{O}\left(\frac{N_{ins}}{\Delta B}\right)$ new internal nodes can be created since each internal node initially has a degree of $\leq \Delta/2$. Each new internal node is created in $\mathcal{O}(\Delta)$ I/O's and it thus costs $\mathcal{O}(N_{ins}/B)$ I/O's to create new internal nodes without the cost of refilling point buffers which we will account for in the following.

An insertion has to be moved at most from the top to the bottom of the tree before it is cancelled or moved into a point buffer. Since the height of the tree is $\mathcal{O}\left(\frac{1}{\epsilon} \log_B N\right)$ it follows that the cost of handling overflowing insertion buffers during the course of $N_{ins}$ insertions becomes $\mathcal{O}\left(\frac{N_{ins}}{B/\Delta}\frac{1}{\epsilon}\log_B N\right)$ I/O's.
A similar argument can be given for the case of deletions.

Each deletion leaves behind a hole which needs to be filled. This hole is filled by recursively pulling up points which effectively moves down the hole. Each split of internal nodes also potentially creates up to $B$ holes.
In total we need to handle $\mathcal{O}\left(N_{del} + \frac{N_{ins}}{\Delta B}\right)$ holes. We can move up $B/2$ points using $\mathcal{O}(\Delta)$ I/O's and they at most need to be moved to the top of the tree, i.e. they need to be at most moved the height of the tree levels up. This gives a total cost of $\mathcal{O}\left(\left(N_{del} + \frac{N_{ins}}{\Delta B}\right)\frac{\Delta}{B}\frac{1}{\epsilon} \log_B N\right)$ I/O's.

All this adds up to $\mathcal{O}\left(\frac{N_{ins}+N_{del}}{B/\Delta} \frac{1}{\epsilon} \log_B N\right) = \mathcal{O}\left(\frac{N_{ins}+N_{del}}{\epsilon B^{1-\epsilon}} \log_B N\right)$ I/O's. This matches the amortized bounds of the structure.

\begin{savequote}[0.5\textwidth]
``The problem with quotes found on the\\internet is that they are often not true.''
\qauthor{--- Abraham Lincoln}
\end{savequote}
\chapter{Other structures}
\label{chp:other_structures}
\section{R-Tree}

The R-Tree was introduced by Antonin Guttman in~\cite{Guttman:1984:RDI:602259.602266}. The structure is heuristic in nature and does not provide any close to optimal worst case search bounds. Arge et al. has, however, provided strong evidence for R-Trees outperforming several theoretical optimal data structures in practice in~\cite{Arge:2008:PRP:1328911.1328920}. We will introduce the important ideas of the R-Tree.

Let $P$ be a set of points. An R-Tree stores all points from $P$ in leaf nodes, each of which contains $\Theta(B)$ points. Each non-leaf node $u$ has $\Theta(B)$ children, except for the root which must have 2 children unless it is the only node in the tree. For each child $v$, $u$ stores a \textit{minimum bounding rectangle} (MBR), which is the smallest rectangle that \textit{tightly} encloses all the points in the subtree of $v$. Note that there is no constraint on how points should be grouped into leaf nodes. Also, there is no constraint on how non-leaf nodes should be grouped in higher level nodes. Since each point is stored only once, the entire data structure consumes linear space. Please refer to Figure~\ref{fig:r_tree_structure} for an illustration of a R-Tree structure.

\begin{figure}[h]
	\centering
     \includegraphics[width=\textwidth]{../figures/r_tree_data_and_mbrs}
     \caption{(a) Data and MBRs (b) The R-Tree structure}
     \label{fig:r_tree_structure}
\end{figure}

\subsection{Query}
Given a query $Q = \left[x_1,x_2\right] \times \left[y_1,\infty\right]$ we want to find all the points in $P$ that are covered by $Q$. The relation to the R-Tree is that we only need to visit those nodes whose MBRs intersect $Q$. Intuitively, this means we desire as \textit{small} MBRs as possible, as this directly implies that a query spans fewer MBRs, again implying fewer nodes are visited. A good heuristic is therefore to minimize the perimeter of each MBR as this directly implies MBRs that covers smaller areas.

\subsection{Insertions}
To insert a point $p$, we add $p$ to a leaf node $u$ by following a single root-to-leaf path. If $u$ overflows we split it, which creates a new child of $parent(u)$. This could cause $parent(u)$ to overflow which is handled in a recursive manner. Finally, if the root split, then a new root is created. Note that it is legal to insert a point $p$ into \textit{any} leaf, after which, the data structure will still be considered legal. This is the main property that differs R-Trees from standard B-Trees. There are several heuristics for choosing a subtree to insert into. It is these heuristics that gives rise to the different R-Tree variants and names. We will cover the original R-Tree heuristic and the R$^*$-Tree heuristic.

The formal definition of inserting a new point $p$ is as follows. Given a non-leaf $u$ with children $v_1, v_2, \cdots, v_{\Theta(B)}$, we need to pick the best child $v^*$ such that the new point $p$ is best inserted into the subtree of $v^*$.

\textbf{Choosing a subtree to insert in an R-Tree.}  The standard R-Tree chooses the best child in a greedy manner. Specifically, $v^*$ is simply the child $v_i$ whose MBR requires the \textit{least increase} of area in order to cover $p$.

\textbf{Choosing a subtree to insert in an R$^*$-Tree.} The problems in the original R-Tree is that certain types of data points may create small areas but large distances which will initiate a bad split. To overcome this, a mixed heuristic is employed. At leaf level we try to minimize the overlap and in case of \textit{ties} the MBR that requires the \textit{least increase} of perimeter is chosen. If this again yields a tie the MBR that increases the least in area is chosen. At the higher levels, it behaves similar to the R-Tree.

\textbf{Node split in an R-Tree} was by Guttman originally proposed done using two different heuristics. The \textit{linear method} chooses far apart nodes as ends. Randomly nodes are then chosen and assigned such that they require the smallest MBR enlargement. The \textit{quadratic method} chooses two nodes such that the dead space between them is maximized. Nodes are then assigned such that the MBR area is minimized.

\textbf{Node split in an R$^*$-Tree is fairly more involved} but the main idea is to always split point set $S$ using an axis-orthogonal cut. This means that points of $S$ is sorted with respect to their $x$- and $y$-value respectively. Then, the first $\nicefrac{B}{2}$ points are inserted into $S_1$ and the rest is inserted into $S_2$ for the $x$-sorted points and into $S'_1$ and $S'_2$ for the $y$-sorted points. The final split is the better one of the two splits, i.e. the splits that have the least combined MBR perimeter and least combined MBR area. The above applies to splitting of a leaf node. The case of a non-leaf node is a bit different because the items split are MBRs. The strategy is however the same and involves sorting the MBRs by their centroids. Please refer to figure~\ref{fig:r_tree_splitting} for an illustration of the splitting of a node.

\begin{figure}[h]
	\centering
     \includegraphics[width=\textwidth]{../figures/r_tree_splitting}
     \caption{(a) Split by cutting the $x$-dimension (b) Split by cutting the $y$-dimension}
     \label{fig:r_tree_splitting}
\end{figure}

\subsection{Deletions}
Let $p$ be the point to be deleted. First the leaf node $u$ which stores $p$ is found using $p$ as search region. Then $p$ is removed from $u$. The deletion is finished if the node has $\lambda B$ points, where $\lambda$ denotes the minimum node utilization. Otherwise, $u$ \textit{underflows}, which is handled by first removing $u$ from its parent, and then re-inserting all points in $u$ using the insert algorithm described earlier. Now, removing $u$ from $parent(u)$ may cause $parent(u)$ to underflow too. In general, the underflow of a non-leaf node $u'$ is also handled by re-insertions, with the only difference that the items re-inserted are MBRs, and each MBR is re-inserted to the same level of $u'$.

\subsection{Analysis}
It follows trivially from construction that a point can be inserted in $\mathcal{O}(\log_B N)$ by simply searching after the MBR that is responsible for the update.

While much work has been done on evaluating the practical query performance of the different variants of the R-Tree, very little is known about their theoretical worst-case performance. Most theoretical work on R-Trees is concerned with estimating the expected cost under hard assumptions on the distribution of input and on the queries that is to be answered.

Since we cannot guarantee the heuristics for constructing the R-Tree choosing all MBRs not to overlap, we believe a worst case analysis for querying must be $\mathcal{O}(N)$.

\section{MySQL}
\label{sec:mysql}
MySQL is a very popular open-source relational database management system. We will not give an in depth description of how relational databases work but we will describe how to very simply adapt MySQL to answer three-sided range queries. A very minimal table was constructed with just two columns. One for the $x$ coordinate and one for the $y$ coordinate. These two columns are base for a primary key on the table. This eliminates duplicates in the table and allows for faster range queries by building some variant of a B-Tree on the concatenation of $x$ and $y$ yielding a single key, i.e. MySQL does not do anything extraordinary to utilize the two elements of the key. Inserting in the table was done using simple \texttt{INSERT IGNORE INTO table VALUES \{ values \}} SQL queries. In order to make this work efficiently we buffer inserts in memory and bulk insert whenever the buffer overflows. This gave a significant speedup. Deletion was done similarly simple using \texttt{DELETE FROM table WHERE (x,y) IN \{ values \}} and buffers on top.

Queries of the form $\left[x_1,x_2\right] \times \left[y_1,\infty\right]$ were answered using a \texttt{SELECT * FROM table WHERE $x_1$ <= x AND x <= $x_2$ AND y >= $y_1$}.

\subsection{Analysis}
It is important to keep an open connection to the server at all times, if not we will end up spending a lot of time reconnecting to the server.

Inserting points in the database involves sending the query to the server, parsing the query, and finally inserting the rows. Due to the primary key on the table, we know that MySQL will build a B-Tree on the data. This will give an insertion time of $\mathcal{O}(\log_B N)$ I/O's for some MySQL implementation specific $B$.

Deleting points is similar to inserting and is also done in $\mathcal{O}(\log_B N)$.

It is a little harder to argue about the complexity of a three sided query. A B-Tree can answer normal two sided range queries in $\mathcal{O}(\log_B N + K/B)$. We can however not guarantee that all points in the two sided range should be reported and thus we cannot properly attribute any I/O's to the output, i.e. use filtering. We will have to settle with a complexity of $\mathcal{O}(\log_B N + T/B)$ where $T$ is the size of the output within the $x$ range of the query.

If we enforce no index on the MySQL table, then we believe the standard implementation will simply append all insertions to a continuous stream, which can be done in $\mathcal{O}(1)$ I/O's. We believe deletions and queries can be handled in $\mathcal{O}(N/B)$ I/O's by scanning the entire stream.

\begin{savequote}[0.7\textwidth]
``There is nothing more deceptive than an obvious fact.''
\qauthor{--- Sherlock Holmes}
\end{savequote}
\chapter{Implementation}
\label{chp:implementation}

Throughout our implementation of the Internal Memory Priority Search Tree, External Memory Priority Search Tree, and the External Memory Buffered Priority Search Tree we noted down important considerations.
In this chapter we present these considerations together with a short presentation of how we wrote wrappers around MySQL, Boost R-Tree, and libspatial R*-Tree. We end the chapter with a description of the experimental framework we developed to significantly simplify the experimental phase of the project.

All code can be cloned from the following git repository or downloaded from the mirrors listed below. Instructions on how to compile and run the code can be found in the accompanying \texttt{readme} file.

\begin{center}
\url{https://github.com/gabet1337/speciale}
\url{http://cs.au.dk/~peterg/three_sided.zip}
\url{http://cs.au.dk/~chrha22/three_sided.zip}
\end{center}

\section{General}
Almost all code was developed using pair programming and we strongly believe this technique, though slow and cumbersome, eliminated many mistakes that would otherwise had slowed us down later on.
Everything we implemented was unit tested and large parts of the project implemented using test driven development. We made sure to make sound design choices to enable easy extension and reuse of our code. Stubs and mocks were used to ease the integration of substructures into larger structures.
We also wrote checkers to automatically check validity of a tree. In the case of the External Memory Buffered Priority Search Tree we wrote a checker that would iterate the entire tree and check that no invariants were broken. We also included a method to print the trees in a DOT format (graph description language) allowing us to visualize every step of the algorithm. This really made debugging easier as it supplied us with a quick overview of the structure.

We optimized the code as much as time would allow using the profiler \texttt{valgrind}.

In order to fully test that there were no errors in our implementation we wrote a random test that would insert thousands of points and test validity of the tree for every insert, delete, and query. We would then repeat this test for several thousand iterations over several days while continuing on other parts of the code.

Using all these methods and tools we feel confident that our code works as intended.

\section{Stream}
The implementations we present makes use of the concept of \textit{streams}. A stream gives access to reading and writing from disk to memory and vice versa. A stream typically manages an internal buffer which is a mirror of a small portion of the disk allowing for fast interaction with that small piece of data. Although the C++ standard library provides several streams that allow for the internal buffer to be managed we introduce an implementation of our own. We denote this stream \texttt{buffered\_stream}. This design choice was made because of the nature of our experiments in which it is of extreme importance that we are able to argue about the exact number of I/O's being used. By introducing a stream of our own we avoid that any undefined behaviour in the standard library implementations gives rise to a potential I/O overhead. Any such I/O overhead would be reflected directly in the overall running time of our implementation. A stream of our own would further more allow us to count the exact number of I/O's being used.

The stream we introduce makes use of the \texttt{read} and \texttt{write} system calls and is equipped with a buffer of size $B$ in internal memory that is maintained on all operations.

There are many different ways to construct a stream. In order to substantiate our choice of using buffering on top of the \texttt{read} and \texttt{write} system calls we conducted some experiments with some different other types of streams:

\begin{itemize}
	\item Direct invocation of the operating system calls \texttt{read} and \texttt{write} that reads\-/writes one item using no buffering mechanism.
	\item The standard library streams \texttt{fread} and \texttt{fwrite} that use a built\--in buffering mechanism that we do not manage.
	\item Direct invocation of the operating system calls \texttt{mmap} and \texttt{munmap} that makes use of the operating systems virtual memory mechanism through demand paging.
\end{itemize}

It is clear that direct invocation of the \texttt{read} and \texttt{write} system calls cannot be better than adding buffering on top, which early experiments without doubt showed. The results were so slow that we had to exclude them.
\begin{figure}[]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../src/experiments/stream_input_speed_experiment_results/2016-04-23.16_00_16/time}
  \caption{Reading 5Gb}
  \label{fig:stream_input_speed}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{../src/experiments/stream_output_speed_experiment_results/2016-04-23.16_07_40/time}
  \caption{Writing 5Gb}
  \label{fig:stream_output_speed}
\end{minipage}
\end{figure}

The results of Figure~\ref{fig:stream_input_speed} and~\ref{fig:stream_output_speed} shows that the \texttt{buffered\_stream} performs similarly to using \texttt{mmap} when reading 5Gb of data while using \texttt{fread} and \texttt{fwrite} is significantly slower.
Figure~\ref{fig:stream_output_speed} surprisingly shows that it is faster to use the virtual memory mechanism to write data. Using virtual memory however also means that we lose some control of when and how data is written to disk, and we lose the control to accurately measure disk I/O's as we have no control of how the operating system schedules I/O's. We would also have a harder time measuring the number of page faults caused by our structures' internal work as every I/O will cause a page fault which will interfere with our measurements. With these considerations in mind we conclude that it makes most sense to use the \texttt{buffered\_stream}.

\section{External Memory Buffered Priority Search Tree}
\label{sec:impl_main_data_structure}
This Section will present important implementation specific design choices for the main data structure of the External Memory Buffered Priority Search Tree by Brodal presented in Section~\ref{sec:main_data_structure}.

In Section~\ref{sec:main_data_structure} we argued how the operations \texttt{insert}, \texttt{delete}, and \texttt{report} are supported using subroutines for handling overflowing deletion buffers, overflowing insertion buffers, split leafs with overflowing point buffers, split nodes of degree $B^\epsilon+1$, and fill underflowing point buffers.

The implementation we present uses the same idea of introducing a delegated subroutine which is responsible for maintaining one event only.

% introduce dependency mechanism (partly load/flush node).
We decided to implement a \textbf{dependency mechanism} on each node that allows for the node to be partly flushed and loaded. This allows us to optimize the number of I/O's being made as we can restrict the load and flush to be done only on needed data. The data for each node naturally groups into a separate stream for the insert buffer, delete buffer, point buffer, meta data on children, and meta data for the node itself.

% introduce event loop
As described in Section~\ref{sec:main_data_structure} we have to enable a mechanism that allows for subroutines to call each other, as one event can cause other events to be risen. If we simply let each subroutine call each other using a na\"{\i}ve call stack we would have to manage that a subroutine can be both caller and callee and so it is responsible for loading and flushing data with respect to the recursion path we are currently on. This would require a complex logical mechanism that does nothing but manage the control flow of the recursion and handling data load and flushing. In order to overcome this we have introduced an \textbf{event loop mechanism} that uses a stack of events to control flushing, loading and calling  proper subroutines. Using this mechanism we are able to predict exactly what data is needed. Furthermore it becomes an easy task to flush all required data before taking further steps in the recursion ensuring optimal use of available main memory.

% cache abstraction
As it is not uncommon to see consecutive events for the same node we have added a \textbf{caching mechanism} that makes sure not to flush data on any nodes used in the previous event if the same nodes and data is required in the current event. This idea of using a simple caching scheme dramatically reduces the number of I/O's required compared against the na\"{\i}ve solution where we load and flush a node completely between each event.

% introduce meta info_file
In order to reduce the number of I/O's required we make sure to store a detailed \textit{view} of the state of each node in a separate \textbf{info file} that is maintained on each subroutine. This enables us to test whether a node is internal or a leaf and if it has any broken invariants we should fix using only a single I/O.

% introduce states
As both event loop and buffer over-/underflow thresholds depends on whether we are currently global rebuilding, linear constructing, reporting or handling updates, we introduce a \textbf{state switch} that is used throughout our implementation to decide which path the recursion should take.

%%%%%%% General conciderations about the tree.
The \textbf{general representation} of buffers makes use of Red-Black search trees (\texttt{set<point>} from the C++ standard library) on totally ordered points. This design choice allows us to retrieve the minimum and maximum element in each buffer in constant time using \texttt{iterator} pointers. Furthermore we are able to traverse each buffer in sorted order in linear time which is needed when computing a subset of points to handle on a buffer overflow and underflow. We are aware that using a search tree comes with the price of a space blow-up, as each node needs a pointer for children and parent, giving a total of $32 \cdot N$ bytes extra compared to a \texttt{std::vector<point>} representation. We maintain the point buffer as two separate search trees totally ordered on the $x$-value and $y$-value respectively. This is needed as we frequently need access to the minimum $y$-value when deciding whether we should insert a point into the point buffer or insert buffer. The \textbf{catalogue structure} containing information about the children of each node is also represented as a Red-Black search tree on interval-start of the children. In detail each catalogue item stores information about the minimum $x$-point, minimum $y$-point, maximum $y$-point and node id for each child. In the \textbf{info file} of each node we maintain a bit on whether the node is a leaf, a virtual leaf\footnote{An internal node with an empty subtree} or an internal node, whether the node currently has an overflowing or underflowing point buffer, insert buffer, delete buffer, and whether the node is currently node degree overflowed. This allow the event loop to identify whether we should fix any broken invariants caused by events in the neighbourhood, using only 1 I/O.

\section{External Memory Priority Search Tree}
Drawing from the experiences gained while implementing the External Memory Buffered Priority Search Tree of Section~\ref{sec:impl_main_data_structure} we decided to once again make use of the event loop to handle our recursion allowing for full control of loading and flushing. This choice also allowed for simple adaptation of the caching mechanism previously described.

The elements of the base B-Tree is a simple type containing a point, a reference to a child, and a bool telling us whether the point has been deleted. The elements are stored in nodes which is just a collection of the simple point types, a reference to a query data structure, and some booleans used in loading and flushing mechanisms. The collection used for the simple point types is a Red-Black tree (\texttt{std::set}). This collection allows us to find the child responsible for a point in logarithmic time using binary search and naturally keeps the points in sorted order w.r.t the $x$-values.

Updates and reporting are done as described in Section~\ref{sec:arge_updates} and~\ref{sec:arge_query} with no remarkably deviations.

\section{Other structures}
We also implemented wrappers around MySQL 5.7.12, Boost 1.60.0 R-Tree using the quadratic method, and libspatialindex 1.8.5 external R*-Tree such that we could use these structures as a Priority Search Tree. These structures are described further in Chapter~\ref{chp:other_structures}. We made sure to implement functionality to disallow duplicates in the structures on top of the basic functionality of the structures.

\section{Experimental framework}
We developed an extensive framework for running experiments in order to automate the process as much as possible.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{../figures/experimental_setup3}
	\caption{(a) An experiment is created by extending a Base class (b) The experiment is added to an experiment queue (c) When a machine becomes available the experiment is run (c) Measures on statistics and time are automatically plotted and finally the motherboard beeper plays a tune to signal that an experiment has been processed.}
	\label{fig:experimental_setup2}
\end{figure}

\begin{wrapfigure}{O}{0.55\textwidth}
\captionsetup{width=0.45\textwidth}
	\centering
		\includegraphics[width=0.45\textwidth]{../figures/experimental_setup_4}
	\caption{(a) A factory produces an instance of a Priority Search Tree (b) Each experiment is run in a delegated thread (c) Measures are recorded on statistics and time.}
	\label{fig:missing}
\end{wrapfigure}

Figure~\ref{fig:experimental_setup2} depicts the flow of an experiment from thought to result. In order to achieve this flow we wrote a framework that would do all the hard work for us such that all we had to do for each experiment was to describe the actual experiment. As an example, to test the time it takes to insert in all the structures all we would have to do was to tell the framework to insert into the Priority Search Tree and measure for each 10 megabytes. The framework would then automatically run the experiment on all the structures one at a time, measure time, I/O's, page faults, and other data structure specific statistics (overflows, splits, etc.), plot all the gathered data as a function of the input size, and finally play a small tune to signal that the experiment had finished. To make sure that no experiment leaves any ungathered memory behind we start each experiment in its own thread such that the memory would be automatically recollected upon termination of the thread and all caches are dropped between each experiment.

The running time was measured using the \texttt{chrono} library's \texttt{high\_resolution\_clock}. It was measured in both seconds and milliseconds to make sure we had all the required data. In most cases it suffices to measure in seconds.

In order to obtain a deeper understanding of the results we chose to measure the number of major page faults generated when using the data structures. This was done using \texttt{perf} - a performance analysis tool for Linux. Finally we measured the number of I/O's by extending our stream with a counter on calls to \texttt{read} and \texttt{write}.

As the machines run on a 32 bit operating systems it is very important to define \texttt{\_FILE\_OFFSET\_BITS=64}. This forces the system to use 64 bit pointers when seeking in files allowing us to handle data sets larger than $2^{32}$ on the machines.

\begin{savequote}[0.55\textwidth]
``We must never make experiments to confirm our ideas, but simply to control them.''
\qauthor{--- Claude Bernard}
\end{savequote}
\chapter{Experimental setup}
\label{chp:experimental_setup}
Running experiments on I/O algorithms is extremely time consuming. In order to compare I/O efficient algorithms against internal memory algorithms we need input sizes that force the internal memory algorithms to store and load data to and from the disk (swapping). The input size to the algorithms can be severely minimized if run on machines with a small amount of internal memory which in turn decrease the running time severely as well.

It is important to mention some considerations when it comes to choice of persistent storage media. In recent years solid state drives (SSD's) have grown increasingly more desirably in terms of price per gigabyte and storage size, but there is still a significant gap in price between electronic SSD's and mechanical HDD's. Disregarding this gap in price, solid state drives clearly outperforms the mechanical hard disk in every notable aspect. Solid state drives allow random access to blocks and diminish the seek time which is considered to be the culprit of the mechanical disks due to the rotational latency.
The rotational latency makes it very important to store data on the disk in consecutive blocks as scattering data blocks across the disk would be detrimental to the performance as each block would have to wait for the rotational latency. As long as the mechanical disk is still as widespread as it is, these culprits of the mechanical disk have to be taken in consideration when designing I/O efficient algorithms.

With these considerations in mind we acquired two very old Dell machines with the specifications outlined in Figure~\ref{fig:pc_specs}.

We used newer machines for some experiments where it made sense. An example of a situation where it made sense was when we experimentally compared the actual running time with the theoretical running time of a single data structure, i.e. check whether the actual running time divided by the asymptotic complexity would give a horizontal line. For these experiments a newer machine would not change the results but rather provide faster results and not take up scarce time on the two Dell machines.

\begin{figure}[h]
\centering
\begin{tabular}{ll}
CPU & Intel(R) Celeron(R) CPU 3.06GHz \\
CPU L1 cache & 16Kb \\
CPU L2 cache & 256Kb \\
RAM & 512Mb DDR 553MHz \\
Disk & Seagate ST3160828AS \\
Disk capacity & 160Gb \\
Disk number of disks & 2 \\
Disk number of heads & 4 \\
Disk RPM & 7200 \\
Disk rotation time & 8.33ms \\
Disk seek time & 8.5ms \\
Disk buffer size & 8192Kb \\
Disk sector size & 512bytes \\
Operating system & 32 bit Ubuntu 14.04 \\
Kernel version & 4.2.0-27-generic \\
Compiler & gcc 4.8.4 with optimization level 2
\end{tabular}
\caption{Specifications of the two Dell machines used for running experiments.}
\label{fig:pc_specs}
\end{figure}

\begin{savequote}[0.4\textwidth]
``Big results require big ambitions.''
\qauthor{--- Heraclitus of Ephesus}
\end{savequote}
\chapter{Our results}
\label{chp:experimental_results}
Many experiments were conducted to evaluate the performance of the structures against each other. In this chapter we present and discuss the most interesting results. Some of the experiments are limited to only run for a fixed amount of time. This was a necessary restriction as the internal memory structures are severely limited when data input is greater than the available main memory. This will be very apparent in many of the presented results.

\section{Parameter tuning}
Both the original and buffered external memory Priority Search Tree by Arge et al.~(Chapter~\ref{chp:arge_pst}) and Brodal (Chapter~\ref{chp:epst}) respectively are parametrised with fanout and buffer size. These parameters are of a very machine dependent nature as larger main memory allows for larger buffer sizes. It is our goal to utilize as much main memory as available in the machine running the data structure. We decided to focus the parameter tuning sorely on the \texttt{insert} operation, since this would allow us to construct data structures with a large amount of data, which again would give rise to interesting experiments for the remaining operations. Put in other words; it is of no interest to achieve a data structure that queries \textit{really} fast if we are unable to construct it with a decent amount of data within a decent amount of time.

In Subsection~\ref{subsec:tuning_gerth} we present our tuning parameters for the structure by Brodal, and in Subsection~\ref{subsec:tuning_arge} we present our findings for the structure by Arge et al.

\subsection{External Memory Buffered Priority Search Tree}
\label{subsec:tuning_gerth}
The experiments on Brodals consisted of inserting 5 Gb of data, i.e. for a $8$ byte point $5 \cdot 1024 \cdot 1024 \cdot 1024 / 8 = 640$ million data points. The coordinates of the data points was uniformly distributed among the positive integers. 

\subsubsection*{Buffer size}

In the experiment conducted on the buffer size we fixed the fanout to two and varied the buffer size. Theoretically the running time should decrease with ever increasing buffer sizes as shown in Figure~\ref{fig:gerth_buffer_size_theory}. We are, however, limited by the main memory size and thus have to be careful not to cause swapping of memory to disk as this greatly decreases performance. 

The results plotted as running time per insert is depicted in Figure~\ref{fig:gerth_buffer_size_experiment}. Clearly the actual running time follows the same tendency as the theoretical number of I/O's per insert. We will focus on the number of I/O's to support this claim. In Figure~\ref{fig:gerth_buffer_size_experiment_ios_per_insert} we plot the actual number of I/O's used per insert. Again it seems we have a good alignment between the actual number and the theoretical number of I/O's. In order to verify the running times to be truly bound by the number of I/O's we plot the actual time per insert divided by the theoretical number of I/O's per insert. Please refer to Figure~\ref{fig:gerth_buffer_size_experiment_time_divided_asymptotic}. We expect a close relation between running time and the theoretical number of I/O's to produce a plot with close-to straight lines. For buffers of sizes 1~Mb, 2~Mb, 4~Mb, 8~Mb, 16~Mb we believe this to be the case. 

For buffer size 32 Mb we see more fluctuations and we can hardly argue the plot follows a horizontal line. We believe this is caused by the limited amount of internal memory forcing us to utilize more than available, which again causes the operating system to swap out internal memory to external memory. 

We verified this by looking at the measured number of page faults generated, and we could see that, just as suspected, the data structure started to cause a low amount of page faults that can explain the fluctuations. See Figure~\ref{fig:page_faults_for_large_buffer_size} in Appendix~\ref{app:data} for the measured number of page faults.

In Section~\ref{sec:impl_main_data_structure} we argued about the high space overhead of using Red-Black trees as buffers. When we underflow a point buffer we must load 2 child structures along with all of the related buffers giving a huge space blow up. These two facts together explains the relatively low threshold at which we begin to generate page faults.

We conclude that while the buffers fit in memory the running time decreases with larger buffer sizes. The results shown in this section was not generated from the machines described in Chapter~\ref{chp:experimental_setup}, but a similar experiment on those machines showed that a buffer size of 8Mb performed best. Going forward we will stick to using a buffer size of 8Mb.

\begin{figure}[htp]
\centering
\includegraphics[width=0.93\textwidth]{../plots/gerth_update_buffer_size/gerth_buffer_size}
\caption{Theoretical asymptotic update time per operation for buffer sizes \\ $B \in \{1 \text{Mb}, 2 \text{Mb}, 4 \text{Mb}, 8 \text{Mb}, 16 \text{Mb}, 32\text{Mb} \}$. Each graph is on the form $f(N) = \frac{1}{\epsilon B^{1-\epsilon}} \log_B N$ for $\epsilon = \log(2) / \log(B)$. An epsilon on this form guarantees a fanout $B^\epsilon = 2$.}
\label{fig:gerth_buffer_size_theory}

\includegraphics[width=\textwidth]{../src/experiments/gerth_buffer_size_experiment_results/2016-05-03.13_51_54/time}
\caption{Experimentally measured update time per operation for buffer sizes \\ $B \in \{1 \text{Mb}, 2 \text{Mb}, 4 \text{Mb}, 8 \text{Mb}, 16 \text{Mb}, 32 \text{Mb} \}$ on the data structure of Brodal with fanout $B^\epsilon = 2$. Clearly the tendencies align with the theoretical update bounds depicted in Figure~\ref{fig:gerth_buffer_size_theory}}
\label{fig:gerth_buffer_size_experiment}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[width=\textwidth]{../src/experiments/gerth_buffer_size_experiment_results/2016-05-03.13_51_54/io_per_insert}
\caption{Experimentally measured I/O's per insert for buffer sizes \\ $B \in \{1 \text{Mb}, 2 \text{Mb}, 4 \text{Mb}, 8 \text{Mb}, 16 \text{Mb}, 32 \text{Mb} \}$ on the data structure of Brodal with fanout $B^\epsilon = 2$. Clearly the tendencies align with the theoretical update bounds depicted in Figure~\ref{fig:gerth_buffer_size_theory} and the actual update times depicted in Figure~\ref{fig:gerth_buffer_size_experiment}.}
\label{fig:gerth_buffer_size_experiment_ios_per_insert}

\includegraphics[width=0.96\textwidth]{../src/experiments/gerth_buffer_size_experiment_results/2016-05-03.13_51_54/time_divided_asymptotic}
\caption{Actual running time per update divided by the theoretical number of I/O's per update. A straight line suggests a close relation between the two measures. The fluctuations for buffer size $B = 32 \text{Mb}$ can be explained by the operating system starting to swap out main memory.}
\label{fig:gerth_buffer_size_experiment_time_divided_asymptotic}
\end{figure}

\clearpage

\subsubsection*{Fanout}
In the experiment conducted on the fanout parameter we fixed the buffer size to 8 Mb and varied the fanout parameter. The readers intuition might deceive him into concluding an ever increasing fanout would cause an ever decreasing update time. This was certainly what we had expected since we pass down an ever decreasing $B / B^\epsilon$ fraction, and each such fraction charges an I/O to the total running time. In Figure~\ref{fig:gerth_fanout_experiment} we present the theoretical update time per operation. We see a clear tendency of ever increasing fanouts causing an ever decreasing performance for $B^\epsilon \in \{4, 5, 6, 8, 16, 32, 64 \}$. When zooming in on the graphs for fanouts $B^\epsilon \in \{2, 3, 4, 5\}$ in Figure~\ref{fig:gerth_fanout_experiment_zoom} we see that no theoretical gain is to be expected when decreasing the fanout from 3 to 2. In fact it seems we achieve the exact same update time per operation for fanout 2 and fanout 4. This is, however, not as surprising as one might think. See Equation~\ref{eq:equal_fanouts_2_4_1} and Equation~\ref{eq:equal_fanouts_2_4_2} where the theoretical bound is shown to be the exact same for fanouts 2 and 4.\\

Let $B^\epsilon = 2$:
\begin{equation} \label{eq:equal_fanouts_2_4_1}
	\begin{split}
		\frac{1}{\epsilon \cdot B^{1-\epsilon}}\log_B N = \frac{2\log_2(B)}{B\log_2(2)}\log_B N \\
	\end{split}
\end{equation}

Let $B^\epsilon = 4$:
\begin{equation} \label{eq:equal_fanouts_2_4_2}
	\begin{split}
		\frac{1}{\epsilon \cdot B^{1-\epsilon}}\log_B N & = \frac{4\log_2(B)}{B\log_2(4)}\log_B N \\
		& = \frac{2\log_2(B)}{B\log_2(2)}\log_B N
	\end{split}
\end{equation}

What Equation~\ref{eq:equal_fanouts_2_4_1} and Equation~\ref{eq:equal_fanouts_2_4_2} essential states is that the amortized cost of sending half as many points down per overflow in a tree of double the height remains the same. Drawing from this conclusion we expect the performance of fanout 2 to be equal to fanout 4.

The results showing the experimentally measured update time per insert on varying fanout is depicted in Figure~\ref{fig:gerth_fanout_experiment_time}. Clearly the actual running time aligns with what the theoretical number of I/O's per update from Figure~\ref{fig:gerth_fanout_experiment} suggests. Being truly I/O bound we expect the theoretical number of I/O's and the actual running time to align well with with the actual number of I/Os per update. We are pleased to see this is in fact the case in Figure~\ref{fig:gerth_fanout_experiment_ios}.

To support our claims of the measured running time and number of I/O's aligning well with theory, we have made a plot dividing the actual result with the expected measure. These plots can be seen in Figure~\ref{fig:gerth_fanout_experiment_ios_divided_asymptotic} for the I/O's and in Figure~\ref{fig:gerth_fanout_experiment_time_divided_asymptotic} for the running time. We conclude there is a close relation between the actual measures and what the theory suggests, since we see close-to straight lines in both plots.

If we zoom in on the measured running time for fanouts 2, 3, 4, 5, and 6 we see minor inconsistencies from what the theory suggests. Comparing the measured running time per update in Figure~\ref{fig:gerth_fanout_experiment_time_zoomed} to the expected number of I/O's per update in Figure~\ref{fig:gerth_fanout_experiment_zoom} we see the tendencies aligns, but the order of the fanouts are inconsistent. For example we would have expected fanout 2 to perform the same as fanout 4, and surely we had expected fanout 3 to perform the best of them all. This is not the case.

We believe these inconsistencies can be explained by the one parameter that is not encapsulated in the I/O model; the amount of internal work done. Theory suggest we see decreasing node degree overflows on increasing fanouts, and in turn we expect less node degree overflows to produce fewer point buffer underflows. These expectations align well with our measures on the total number of point buffer underflows depicted in Figure~\ref{fig:gerth_fanout_experiment_pbu}. But what the I/O model does not account for is the internal work needed for handling the actual point buffer underflow. In fact we regard this exact operation to be the most expensive with regard to internal work. 
Running a profiler on the structure for different fanouts showed us the exact distribution of time spent in different functions of the code. The profiler showed that for smaller fanouts we spent more time doing point buffer underflows, relative to the other operations, than we do for larger fanouts. Refer to Figure~\ref{fig:brodal_pb_underflow} for a detailed description of the underflow procedure. What we claim is that in the area of fanouts from 2-6, for the smaller of the fanouts, we are dominated by the internal work needed for handling the many point buffer underflows such that we do not benefit from the large $B / B^\epsilon$ fraction of points we pass down on each overflow.

The results would suggest that a fanout of size 5 would give the best performance.
The results presented was, however, not generated from the machines presented in Chapter~\ref{chp:experimental_setup}. The results of these machines show that with a buffer size of 8 Mb it would be best to use a fanout size of 2. As the rest of the experiments will be run on the machines described in Chapter~\ref{chp:experimental_setup} we will use a fanout of size 2 going forward.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../plots/gerth_update_epsilon/gerth_epsilon}
  \caption{Theoretical asymptotic update time per operation for fanouts $B^\epsilon~\in~\{2, 3, 4, 5, 6, 8, 16, 32, 64\}$. Each graph is on the form $f(N)~=~\frac{1}{\epsilon B^{1-\epsilon}} \log_B N$ for $\epsilon~=~\log(fanout) / \log(B)$. An epsilon on this form guarantees the desired fanout.}
  \label{fig:gerth_fanout_experiment}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../plots/gerth_update_epsilon/gerth_epsilon_zoom}
  \caption{Zoomed-in plot of Figure~\ref{fig:gerth_fanout_experiment} depicting the asymptotic update time per operation for fanouts $B^\epsilon~\in~\{2, 3, 4, 5, 6\}$. Note that fanouts 2 and 4 gives rise to the exact same graphs. This can be explained by Equation~\ref{eq:equal_fanouts_2_4_1} and~\ref{eq:equal_fanouts_2_4_2}.}
  \label{fig:gerth_fanout_experiment_zoom}
\end{minipage}
\centering
\includegraphics[width=\textwidth]{../src/experiments/gerth_fanout_experiment_results/2016-05-06.11_52_24/time}
\caption{Experimentally measured update time per operation for fanouts $B^\epsilon~\in~\{2,3,4,5,6,8,16,32,64\}$ on the data structure of Brodal with buffer size $B = 8 Mb$. Clearly the tendencies align with the theoretical update bounds depicted in Figure~\ref{fig:gerth_fanout_experiment}.}
\label{fig:gerth_fanout_experiment_time}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{../src/experiments/gerth_fanout_experiment_results/2016-05-06.11_52_24/ios}
\caption{Experimentally measured number of I/O's per insert as a function of input size on the structure of Brodal with a fixed buffer size of 1 Mb and varying fanouts $B^\epsilon~\in~\{2,3,4,5,6,8,16,32,64\}$. Clearly the tendencies align with the theoretical update bounds depicted in Figure~\ref{fig:gerth_fanout_experiment}.}
\label{fig:gerth_fanout_experiment_ios}
\begin{minipage}[t]{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../src/experiments/gerth_fanout_experiment_results/2016-05-06.11_52_24/ios_divided_asymptotic}
  \caption{Experimentally measured number of I/O's divided by the theoretical number of I/O's per update. A straight line suggests a close relation between the two measures.}
  \label{fig:gerth_fanout_experiment_ios_divided_asymptotic}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../src/experiments/gerth_fanout_experiment_results/2016-05-06.11_52_24/time_divided_asymptotic}
  \caption{Experimentally measured running time per update divided by the theoretical number of I/O's per update. A straight line suggests a close relation between the two measures.}
  \label{fig:gerth_fanout_experiment_time_divided_asymptotic}
\end{minipage}
\end{figure}

\begin{figure}[]
\includegraphics[width=\textwidth]{../src/experiments/gerth_fanout_experiment_results/2016-05-06.11_52_24/time_zoomed}
\caption{Zoomed-in plot of experimentally measured update time per operation for fanouts $B^\epsilon~\in~\{2,3,4,5,6\}$. Comparing these to the theoretical number of I/O's per update depicted in Figure~\ref{fig:gerth_fanout_experiment_zoom} we see minor divergences. This can be explained by the internal work done on point buffer underflows.}
\label{fig:gerth_fanout_experiment_time_zoomed}
\includegraphics[width=\textwidth]{../src/experiments/gerth_fanout_experiment_results/2016-05-06.11_52_24/pbu}
\caption{Experimentally measured number of point buffer underflows. Clearly there is a tendency to ever decreasing point buffer underflows for ever increasing fanouts.}
\label{fig:gerth_fanout_experiment_pbu}
\end{figure}

\clearpage

\subsubsection*{Summary}

We conclude from our experiments with different fanouts and buffer sizes that in order to get as fast an update time as possible we need a fanout of between 2-6 and as large a buffer size as internal memory allows.

It is also worth noting that since the experimental results are very much in line with the theoretical results it can be concluded that the data structure is I/O bound.

Going forward we will use a buffer size of 8Mb together with a fanout of 2. 

\subsection{External Memory Priority Search Tree}
\label{subsec:tuning_arge}

The experiments on Arge's consisted of inserting 50 Mb of data, i.e. $5 \cdot 50 \cdot 1024 \cdot 1024 / 8 = 6.25$ million datapoints. The coordinates of the data points was uniformly distributed among the positive integers. 

The results of the experiment measuring running time per update is depicted in Figure~\ref{fig:arge_buffer_size_experiment}. Clearly we are very far away from the expected update time per operation depicted in Figure~\ref{fig:arge_buffer_size_theory}. The main reason behind this is most likely that more data is needed in the structure for the theoretical $\mathcal{O}(\log_B N)$ I/O per update to be apparent. It would seem we are dominated by the fact we need to load and store $B$ data points on each node visited in order to handle an update. Since the experiments on Arge was very time consuming, we decided to elect the buffer size of 4 Kb as the winning buffer size. We will elaborate more on the main bottlenecks of Arge's structure in the insert experiment found in Section~\ref{sec:insertion}.

\begin{figure}[h]
\centering
\begin{minipage}[t]{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../plots/arge_complexities/arge_complexity}
  \caption{Theoretical asymptotic update time per operation for buffer sizes $B~\in~\{4\text{Kb},8\text{Kb},16\text{Kb},32\text{Kb}\}$ for Arge's structure. Each graph is on the form $f(N) = \log_B N$.}
  \label{fig:arge_buffer_size_theory}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../src/experiments/arge_buffer_size_experiment_results/2016-04-28.10_03_24/time}
  \caption{Experimentally measured update time per operation for buffer sizes $B~\in~\{4\text{Kb},8\text{Kb},16\text{Kb},32\text{Kb}\}$. Note the relatively long running time needed for inserting 50 Mb data.}
  \label{fig:arge_buffer_size_experiment}
\end{minipage}
\end{figure}

\clearpage

\section{Insertion}
\label{sec:insertion}
The goal of this experiment is to ascertain how the different structures compare against each other when it comes to inserting points. The experiment consist of inserting as much data as possible within 24 hours. The data is uniformly randomly distributed in the positive integer range.

Figure~\ref{fig:theory_insert_complexity} shows the theoretical complexities of inserting an element into the different structures for different input sizes. We expect the data structures not optimized for external memory to align close to the theoretical bounds while they are completely contained in main memory. Only when the operating system is forced to swap data between internal and external memory do we expect a significant decrease in performance. When the data structures are no longer able to fit into main memory, and since they all rely on scattered data access, do we expect an amount of page faults close to the theoretical asymptotic complexity. For the external memory data structures would we expect the MySQL implementation without any index to outperform all of the other structures, since inserting essentially appends points to a single stream. We expect the effective buffering of points in the External Memory Buffered Priority Search Tree by Brodal to outperform the R-Tree variants and MySQL with an index on coordinates.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{../plots/insert_complexities/insert_complexity}
\caption{Theoretical asymptotic insert time for all tested structures. Brodal is on the form $f(N) = \frac{1}{B^{1-\epsilon}} \log_B N$ for $\epsilon = \frac{\log(B)}{\log(2)}$. Arge and MySQL (with index) is on the form $f(N) = \log_B N$. The Internal Memory Priority Search Tree is on the form $f(N) = \log_2 N$. The MySQL (no index), Boost R-Tree and Libspatial R*-Tree are on the form $f(N) = N$.}
\label{fig:theory_insert_complexity}
\end{figure}

Figure~\ref{fig:actual_insert_time} shows the actual running time per inserted megabyte in all of the tested structures. The figure is cropped at $N = 600$ to better present the relation between the internal memory and the external memory structures. MySQL without an index was able to insert more than 10Gb in less than 3 hours and since there was no change in running time per inserted megabyte did we stop the experiment before the time limit. The External Memory Buffered Priorty Search Tree by Brodal was able to insert around 3.5Gb worth of data within the time limit.

The internal memory data structures performs very well while contained in memory. It can be seen in Figure~\ref{fig:insert_page_faults}, depicting the number of page faults generated by the internal data structures, that there is a close relation between the running time and the number of page faults. Clearly we see the running time starting to increase significantly at the same input size as where the number of page faults increases.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../src/experiments/insert_experiment_results/combined_results/time_over_asymptotic}
\caption{Actual running time when inserting into the tested structures. The figure shows the running time per insert cropped to the first 600Mb.}
\label{fig:actual_insert_time}
\end{figure}

\begin{figure}[b]
\centering
\includegraphics[width=\textwidth]{../src/experiments/insert_experiment_results/combined_results/pfs}
\caption{Number of page faults for the internal memory data structures.}
\label{fig:insert_page_faults}
\end{figure}

%Figure~\ref{fig:actual_insert_time} shows the actual running time of inserting an element into the different structures for different input sizes. 
%The figure is cropped at 3500Mb but it goes all the way to 10000Mb. It was however only MySQL that was able to insert this much data in 24 hours.
%The results are very much in line with our expectations with some exceptions. We see significant increases in running time whenever the internal structures run out of memory which is after just a few hundred Mb's of insertions. The internal R-tree from the Boost library was the most efficient of the internal data structures. We suspect this is due to an efficient memory usage allowing the structure to run for longer while still fitting entirely in main memory.

%The results of MySQL and the External Memory Buffered Priority Search tree by Brodal align very well with the theoretical bounds, but this is clearly not the case for the structure of Arge et al. 

It is obvious that the data structure of Arge performs the worst of all the data structures when it comes to inserting data. We believe this is due to the huge amount of data we need to load and store for inserting just a single point. We will argue this in a more precise manner. It follows from the theory that a single insert requires $\mathcal{O}(\log_B N)$ I/O's. Each data point uses 8 bytes of space, 4 bytes of each coordinate. This means that in order to insert 50Mb data or equivalently 6,553,600 points in an initially empty structure with a buffer size of $4Kb$ we need roughly $$\sum\limits_{i=0}^{6553600} \log_{4096}(i) \approx 11.5 \cdot 10^6~\text{I/O's}$$
Comparing this against the structure of Brodal with the same buffer size and a fanout of size 2, we get that Brodal's requires roughly:
$$\sum\limits_{i=0}^{6553600} \frac{1}{\epsilon \cdot 4096^{1-\epsilon}} \log_{4096}(i) = 67.8\cdot 10^3~\text{I/O's}$$

The calculations shows that in order to insert 50Mb of data, we need to load/store $11.5 \cdot 10^6 \cdot 4096 = 47.1Gb$ in the case of Arge, and $67.8\cdot 10^3 \cdot 4096 = 0.278Gb$ in the case of Brodal. This gives a relatively difference of $\approx 170$ times fewer I/O's performed in Brodal's over Arge's.

To collaborate these observations we conducted a single experiment measuring the I/O's used when inserting 50Mb of data. The results can be found in Figure~\ref{fig:arge_brodal_ios}. The results clearly shows that the amount of I/O's performed is substantially larger for Arge's than for Brodal's. In fact we can see that in order to insert 50Mb we must perform I/O's equal to moving around $1Tb$ and around $13Gb$ of data in Arge's and Brodal's respectively. This is a relative difference of $\approx 81$ times fewer I/O's in Brodal's compared with that of Arge's. This result is close to what we would suspect from the theoretical reasoning above, however with a large constant factor between theory and reality.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{../src/experiments/insert_experiment_results/2016-05-21.14_06_50/ios}
\caption{The number of I/O's for the structure of Arge and Brodal respectively. The y-axis on the right shows the relative difference in terms of I/O's between Arge and Brodal. It shows that Arge does 81 times more I/O's than Brodal when inserting 50Mb.}
\label{fig:arge_brodal_ios}
\end{figure}

\begin{figure}
\centering
\begin{minipage}{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../src/experiments/insert_experiment_results/2016-05-22.16_20_22/time_and_ios}%
  \caption{Time and I/O's per insert divided by $\frac{1}{\epsilon B^{1-\epsilon}}\log_B(N)$ for the structure of Brodal.}
  \label{fig:time_divided_with_asymptotic_brodal}
\end{minipage}%
\hfill
\begin{minipage}{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../src/experiments/insert_experiment_results/2016-05-25.10_23_00/time_and_ios}%
  \caption{Time and I/O's per insert divided by $\log_B(N)$ for the structure of Arge et al.}
  \label{fig:time_divided_with_asymptotic_arge}
\end{minipage}

\begin{minipage}{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../src/experiments/insert_experiment_results/2016-05-27.12_06_22/time_and_pfs}%
  \caption{Time and page faults per insert divided by $\log_2(N)$ for the Internal Memory Priority Search Tree of McCreight.}
  \label{fig:time_divided_with_asymptotic_internal}
\end{minipage}%
\hfill
\begin{minipage}{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../src/experiments/insert_experiment_results/2016-05-27.10_58_31/time_and_pfs}%
  \caption{Time and page faults per insert divided by $N$ for the Boost R-Tree.}
  \label{fig:time_divided_with_asymptotic_boost}
\end{minipage}

\begin{minipage}{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../src/experiments/insert_experiment_results/2016-05-27.13_39_38/time_and_pfs}%
  \caption{Time and page faults per insert divided by $N$ for the libspatial external R*-Tree.}
  \label{fig:time_divided_with_asymptotic_libspatial}
\end{minipage}%
\hfill
\begin{minipage}{0.48\columnwidth}
  \centering
  \includegraphics[width=\linewidth]{../src/experiments/insert_experiment_results/combined_results/time_with_and_without_mysql}%
  \caption{Time per insert divided by $1$ for MySQL without an index and time per insert divided by $\log_B N$ for MySQL with an index.}
  \label{fig:time_divided_with_asymptotic_mysql}
\end{minipage}
\end{figure}

Figure~\ref{fig:time_divided_with_asymptotic_brodal}-\ref{fig:time_divided_with_asymptotic_mysql} shows the time per insert divided by the theoretical asymptotic bound of each of the tested structures. In addition we also display the number of I/O's per insert divided by the theoretical asymptotic bound for the internal memory data structures while the results of the main memory structures also include the page faults per insert divided by the theoretical asymptotic bound. 

If the actual running time match the theoretical bounds then we would like to see that the graphs forming horizontal lines. The caveat is that the internal memory data structures will start to swap out data when they have no more free main memory to use, which again causes the running time to increase severely. If we see a spike in both running time and number of page faults at the same mark then we feel confident that the to measures are closely related.











In Figure~\ref{fig:time_divided_with_asymptotic_brodal} we see that the time per insert in the structure of Brodal comes closer and closer to the theoretical bound. This can be seen by the flattening of the graph as the input size goes up. This would suggest that the structure becomes more and more I/O bound as the input size goes up. The structure follows the same trend in terms of I/O which further strengthens this hypothesis.

In Figure~\ref{fig:time_divided_with_asymptotic_arge} we see that the time per insert and number of I/O's per insert follows the theoretical bound very well up to a point. We investigated this point further and saw that this was the exact moment when the structure went from 2 to 3 layers. Adding another layer significantly increased the amount of I/O's. In terms of I/O's we see that the curve flattens once again which shows that the number of I/O's once again become a constant factor of the theoretical bounds. The same cannot be said for the actual running time which does not follow the same trend. We would have liked to see that it would flatten just as the number of I/O's but this is not the case which suggest that the structure is still heavily CPU bound at this point.

Figure~\ref{fig:time_divided_with_asymptotic_internal} shows that the Internal Memory Priority Search Tree of McCreight performs exactly as expected as long as the structure is contained in main memory. At the exact moment the structure starts to swap out main memory we see a significant increase in running time just as expected.

It is almost the same picture for the Boost R-Tree in Figure~\ref{fig:time_divided_with_asymptotic_boost}. The insertion time in an R-Tree heavily depends on heuristics which explains the decreasing graph and far from worst case behaviour. Once again we see the same behaviour when we run out of main memory -- the running time suffers tremendously.

The libspatial external R*-Tree depicted in Figure~\ref{fig:time_divided_with_asymptotic_libspatial} shows that the running time follows the expected theoretical bounds very well. We would have liked to measure the number of I/O's for this structure but since it is a library implementation we have not been able to do this. The good news is that it does not generate any page faults.

Figure~\ref{fig:time_divided_with_asymptotic_mysql} shows the both the running time of MySQL with an index and without and index divided by the asymptotic running time, which for the case of the indexed version is $\mathcal{O}(\log_B N)$ and the non-indexed version is $\mathcal{O}(1)$. The results show that MySQL without an index follows the asymptotic running time very closely while the indexed version becomes steeper and steeper. Digging into the MySQL implementation of indices and B-Tree's we found that MySQL uses a Red-Black tree to store data in main memory and then bulk insert the sorted data into a B-Tree whenever the Red-Black tree overgrows main memory. We believe the increases in running time is caused by this bulk unloading into a B-Tree\footnote{\url{https://dev.mysql.com/doc/internals/en/bulk-insert.html}}. We deemed it out of scope for this thesis to look closer into the inner workings of the MySQL index structure.

\clearpage

\section{Deletion}
It was difficult to come up with a good way to test the different structures against each other when it comes to deletion. In order to delete data we need to insert it first, and we need to insert equally much data in all structures in order to compare fairly. As described in the previous section it was not possible to insert much data in the internal structures as well as the structure of Arge et al. To come around this problem we decided to insert nothing more than 50 megabytes worth of uniformly distributed points in all the structures. We could then completely empty the structures again while measuring for each megabyte. This way we hoped we could exclude structures from further analysis.

\begin{figure}[htp!]
\centering
\includegraphics[width=1\textwidth]{../plots/delete_complexities/delete_complexity}
\caption{Theoretical asymptotic delete time for all tested structures. Brodal is on the form $f(N) = \frac{1}{B^{1-\epsilon}} \log_B N$ for $\epsilon = \frac{\log(B)}{\log(2)}$. Arge and MySQL (with index) is on the form $f(N) = \log_B N$. The Internal Memory Priority Search Tree is on the form $f(N) = \log_2 N$. The MySQL (no index), Boost R-Tree and Libspatial R*-Tree are on the form $f(N) = N$.}
\label{fig:theory_delete_complexity}
\end{figure}

Figure~\ref{fig:theory_delete_complexity} shows the theoretical complexity for deletion in the structures. We expect the internal memory algorithms to outperform the external structures significantly while still in main memory. As soon as we leave main memory we expect them to become obsolete. In external memory we expect the structure by Brodal to be the best due to the effective buffering of deletes.

\subsection*{First results}

The results of the initial experiment where we inserted 50Mb and then completely emptied the data structure is depicted in Figure~\ref{fig:delete_complexity_result}. We have left out the results from the structure of Arge since we were able to delete less than 2Mb worth of data within 24 hours, and thus concluded it would be infeasible to finish the experiment within reasonable time. We believe the poor performance of Arge's structure is due to similar reasons as described in Section~\ref{sec:insertion} where we explained how a huge amount of I/O's and a significant amount of internal work slowed down the structure.

In Figure~\ref{fig:delete_complexity_result} we see some, at first glance, strange behaviour on the running time of the Internal Memory Priority Search Tree. It seems we are achieving ever increasing running times on the first 9Mb worth of deletions, and then, after seeing a significant spike, we are suddenly achieving very good running times on the emptying of the rest of the data structure. This is, however, not as surprising at it might seem. Remember, we are making use of global rebuilding in our implementation of the Internal Memory Priority Search Tree. We do this by \textit{marking} any deleted place-holder, instead of \textit{removing} the actual place-holder from the data structure. This is also known as a \textit{weak} delete of an element. Only when we initiate a global rebuild on the non-deleted points are we freeing the occupied memory of deleted elements~\footnote{We could, without having broken correctness of the implementation, have deleted place-holders, but that was an observation made in hindsight, and we did not have the time to make this change in code.}. So what we are seeing are measures on an ever decreasing data structure in terms of non-deleted data. But the data is still part of the tree, and thus it affects space usage, and again running time of our delete procedure. Since we initiate a global rebuild after having processed $\bar{N} / 2$ update operations, where $\bar{N}$ is the number of non-deleted points in our data structure at the start of an epoch, we are global rebuilding exactly around the spike at the 41Mb mark. We claim that the Internal Memory Priority Search Tree on 50Mb data is unable to fit in main memory, and what we see is the effect of the swapping of main memory to disk. Only when we global rebuild to a data structure holding around 40Mb worth of data are we able to process the data structure entirely in main memory. We believe Figure~\ref{fig:delete_page_faults} depicting the number of measured page faults supports this claim.

\subsection*{Narrowing the field}

Clearly Arge's structure, the Libspatial R*-Tree, and the Internal Memory Priority Search Tree performs the worst of all the data structures when it comes to handling deletions. Excluding these gives rise to Figure~\ref{fig:delete_complexity_result_zoomed}. It is obvious that, even with an index on coordinates, the MySQL implementations are performing much worse than the Boost R-Tree and the data structure of Brodal. We suspect the good performance of the Boost R-Tree is due to the effective space-usage and thus the data structure is able to process the deletions entirely in main memory. In order to verify this, we re-ran the experiment only on Brodal's data structure and the Boost R-Tree on larger input.

\subsection*{Finding the winning data structure}

The result of emptying Brodal's and the Boost R-Tree from a size of 400 Mb down to a size of 350Mb worth of data is depicted in Figure~\ref{fig:delete_complexity_result_400Mb}. Clearly we now see the data structure of Brodal's handling deletions several orders of magnitude faster than the Boost R-Tree. We claim this is caused by the fact that the Boost R-Tree cannot fit into main memory, and thus have to rely on the operating system handling swapping of data. We believe Figure~\ref{fig:page_faults_boost_r_tree} in Appendix~\ref{app:data} depicting the number of page faults for the experiment supports this claim.

\subsection*{Explaining fluctuations}

Finally we zoom in on the result of completely emptying Brodal's data structure from a size of 400 Mb of data. The result is depicted in Figure~\ref{fig:delete_complexity_result_brodal_400mb}. Clearly we see severe fluctuations in running time during the emptying of the data structure. We believe these fluctuations are perfectly explained by the number of point buffer underflows depicted in Figure~\ref{fig:delete_pbu_time_result_brodal_400mb} and the number of delete buffer overflows depicted in Figure~\ref{fig:delete_dbo_time_result_brodal_400mb}.

\begin{figure}[htp!]
\centering
\includegraphics[width=\textwidth]{../src/experiments/delete_experiment_results/2016-05-27.10_10_14/time2}
\caption{Experimentally measured running time when deleting 1 Mb worth of data as a function of the remaining data in the structure going from 50Mb to 0Mb.}
\label{fig:delete_complexity_result}
\includegraphics[width=\textwidth]{../src/experiments/delete_experiment_results/2016-05-27.10_10_14/pfs2}
\caption{Experimentally measured page faults when deleting 1 Mb worth of data as a function of the remaining data in the structure going from 50Mb to 0Mb.}
\label{fig:delete_page_faults}
\end{figure}

\begin{figure}[htp!]
\centering
\includegraphics[width=\textwidth]{../src/experiments/delete_experiment_results/2016-05-27.10_10_14/time4}
\caption{Experimentally measured running time when deleting 1 Mb worth of data in a data structure holding from 50Mb to 0Mb non-deleted data. Clearly the Boost R-Tree performs the best as it can handle all deletions in main memory.}
\label{fig:delete_complexity_result_zoomed}
\includegraphics[width=\textwidth]{../src/experiments/delete_experiment_results/2016-05-31.08_55_51/time2}
\caption{Experimentally measured running time when deleting 1 Mb worth of data in a data structure holding from 400Mb to 350Mb of non-deleted data. Clearly the Boost R-Tree now suffers from swapping of data (see Figure~\ref{fig:page_faults_boost_r_tree}).}
\label{fig:delete_complexity_result_400Mb}
\end{figure}

\begin{figure}[h]
\centering

\includegraphics[width=0.98\textwidth]{../src/experiments/delete_experiment_results/2016-05-31.08_55_51/time3}
\caption{Experimentally measured running time when emptying Brodal's from a size of 400 Mb data. Clearly we see fluctuations in the running time. These are perfectly explained by the number of point buffer underflows depicted in Figure~\ref{fig:delete_pbu_time_result_brodal_400mb} and the number of delete buffer overflows depicted in Figure~\ref{fig:delete_dbo_time_result_brodal_400mb}.}
\label{fig:delete_complexity_result_brodal_400mb}

\includegraphics[width=0.98\linewidth]{../src/experiments/delete_experiment_results/2016-05-31.08_55_51/pbu_time}
\caption{Experimentally measured number of point buffer underflows when emptying Brodal's from a size of 400 Mb data.}
\label{fig:delete_pbu_time_result_brodal_400mb}

\includegraphics[width=0.98\linewidth]{../src/experiments/delete_experiment_results/2016-05-31.08_55_51/dbo_time}
\caption{Experimentally measured number of delete buffer overflows when emptying Brodal's from a size of 400 Mb data.}
\label{fig:delete_dbo_time_result_brodal_400mb}

\end{figure}

\clearpage

\section{Three-sided range queries}
Since the analysis of the theoretical query bounds are done using the method of filtering, we conduct experiments that focuses on both the \textit{search} and the \textit{report} part of the algorithms. Please refer to Section~\ref{sec:filtering} for a description of the method of filtering.

\subsection{Focus on searching}
The experiment on the \textbf{search} part of the algorithms was conducted by first inserting data \textit{inside} 5 fixed query windows of 5 Mb data each, i.e. each query window contains 655.360 points. This was followed by inserting uniformly randomly distributed data \textit{outside} the fixed query windows. For insertion of every 10 Mb of data we report all points inside the fixed query windows giving a total of 25 Mb reported data. The experiment was time limited to run for exactly 24 hours for each of the implementations. We make sure to use query windows such that we report on both small and large ranges and high and low in the tree. The idea is to focus the experiment on measuring \textit{searching} by fixing the output to a constant number of points. Please refer to Figure~\ref{fig:experiment_query_uniform} for an illustration of the query ranges.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.80\textwidth]{../figures/query_uniform}
	\caption{Distribution of points for the uniform reporting experiment (search). The gray areas (a) - (e) contains 5 Mb data each. Points are distributed uniformly random. The query window (a) spans 5\% of the $x$-axis and 100\% of the $y$-axis (b) spans 17\% of the $x$-axis and 20\% of the $y$-axis (c) spans 11\% of the $x$-axis and 60\% of the $y$-axis (d) spans 14\% of the $x$-axis and 40\% of the $y$-axis (e) spans 8\% of the $x$-axis and 80\% of the $y$-axis.}
	\label{fig:experiment_query_uniform}
\end{figure}

All queries are performed once before the actual measuring is done. This is to remove fluctuations on the internal data structures and to remove the amortization from fixing up Brodals. We make sure to stop all clocks when inserting such that our measures only reflect the actual query time.

%Please note that the plot is only intended to show tendencies isolated to each of the data structures, meaning no relative tendencies  are explained by the plot.
The theoretical search times on each of the data structures are depicted in Figure~\ref{fig:theory_query_complexity}. Even though the theoretical asymptotic bound suggests the R*-Tree to be linear in the size of the input, we expect it to have an actual average running time closer to $\mathcal{O}(\log_B N)$ guaranteed when no bounding boxes overlap each other. We expect the external memory data structures to perform much better than the internal memory data structures when the operating system starts swapping of main memory. \todo{ALSO RUN EXPERIMENT ON ASTERIX FOR INDEXED MYSQL}

The result of conducting the experiment focusing on the search part of query on the data structures is depicted in Figure~\ref{fig:result_query_search_complexity}.

Clearly both the Internal Memory Priority Search Tree and the Boost R-Tree suffers when data cannot fit into main memory. We will argue the rest of our observations according to the zoomed-in plot depicted in Figure~\ref{fig:result_query_search_complexity_zoom}. We believe the libspatial R*-Tree follows our expectations of an average $\mathcal{O}(\log_B N)$ search time. Clearly Arge's performs best of all structures, but since we are unable to insert more than around 70 Mb data in the data structure within 24 hours, we must conclude the implementation to be of no practical use when querying large data sets. Clearly the non-indexed MySQL implementation is linear in the input as expected. It is interesting to see the data structure of Brodal perform very well even for a small fanout. The data structure very much aligns with the tendencies suggested by the theory except for minor fluctuations. These are, however, perfectly explained by the insert buffer overflow, node degree overflow, and point buffer underflow needed during fixup. See Figure~\ref{fig:result_query_search_fixup}.

We decided to run a separate experiment on various fanouts on Brodals, since theory claims the data structure to search ever faster on ever increasing fanouts.
Note that we ran this experiment on a different machine. This is not a problem since we are aiming at experimenting on the relative performance between different fanouts opposed to the relative performance between different data structures. The results of these experiments is depicted in Figure~\ref{fig:gerth_query_fanout_experiment}.
Clearly we see a tendency of faster searches on increasing fanouts. \todo{RERUN THIS EXPERIMENT ON ASTERIX}.

\clearpage

\begin{figure}[H]
\centering
\includegraphics[width=0.94\textwidth]{../plots/query_complexities/query_complexity}
\caption{Theoretical search times for the query operation on all structures. Brodals is on the form $f(N) = \frac{1}{\epsilon}\log_B N$ for $\epsilon = \log(fanout) / \log(B)$ and $B = 32$. The data structure of Arge is on the form $f(N) = log_B N$ for $B = 32$. The Internal Memory Priority Search Tree is on the form $f(N) = \log_2 N$. Both the Boost R-Tree, the non-indexed MySQL implementation and the Libspatial R*-Tree are worst case linear in the size of the input, i.e. $f(N) = N$.}
\label{fig:theory_query_complexity}
\includegraphics[width=\textwidth]{../src/experiments/query_experiment_results/final2/time}
\caption{Experimentally measured running time for querying according to Figure~\ref{fig:experiment_query_uniform} on all data structures of size $N$.}
\label{fig:result_query_search_complexity}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../src/experiments/query_experiment_results/final2/time_zoom}
\caption{Zoomed-in version of the experimentally measured running time for querying according to Figure~\ref{fig:experiment_query_uniform} on all data structures of size $N$.}
\label{fig:result_query_search_complexity_zoom}
\includegraphics[width=\textwidth]{../src/experiments/query_experiment_results/final2/all_operations}
\caption{Experimentally measured number of insert buffer overflows, node degree overflows, and point buffer underflows on the data structure of Brodal with fanout 2 depicted in Figure~\ref{fig:result_query_search_complexity_zoom}}
\label{fig:result_query_search_fixup}
\end{figure}

\begin{figure}[htp!]
\centering
\includegraphics[width=\textwidth]{../src/experiments/gerth_query_fanout_experiment_results/2016-05-19.19_04_49/time}
\caption{Experimentally measured running time for querying according to Figure~\ref{fig:experiment_query_uniform} on fanouts $B^\epsilon \in \{4, 16, 32 \}$.}
\label{fig:gerth_query_fanout_experiment}
\end{figure}

\clearpage

\subsection{Focus on reporting}

The experiment on the \textbf{report} part of the algorithms was conducted by fixing $x_1$, $x_2$ and $y_1, \cdots y_n$ and inserting data points such that there is exactly 
1 Mb data points in the range $[x_1, x_2] \times [y_i, y_{i+1}]$. In the range outside  $[x_1, x_2] \times [- \infty, \infty]$ we distribute data points uniformly at random. Now we report points in ranges $[x_1, x_2] \times [y_1, \infty]$, $[x_1, x_2] \times [y_2, \infty]$ $\cdots$ $[x_1, x_2] \times [y_n, \infty]$. The idea is that the number of reported points $K$ grows with 1 Mb when reporting in the range $[x_1, x_2] \times [y_i, y_{i+1}]$ compared to reporting in the range $[x_1, x_2] \times [y_i, y_{i}]$.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/query_uniform_fix_x}
	\caption{Distribution of points for the uniform reporting experiment (reporting). The points $x_1, x_2$ and $y_1, \cdots y_n$ are fixed. There is 1 Mb data in all ranges $[x_1, x_2] \times [y_i, y_{i+1}]$}.
	\label{fig:experiment_query_uniform_fix_x}
\end{figure}

%The expected query bounds for searching is depicted in Figure~\ref{fig:theory_query_complexity}.

The result of the experiment on the \textbf{search} part of the algorithms is depicted in Figure~\ref{fig:result_query_search_complexity}. Arge's structure is limited by the ???

\begin{savequote}[0.4\textwidth]
``To succeed, jump as quickly at opportunities as you do at conclusions.''
\qauthor{--- Benjamin Franklin}
\end{savequote}
\chapter{Conclusion}
\label{chp:conclusion}

We have implemented three different Priority Search Trees: the Internal Memory Priority Search Tree by McCreight~\cite{DBLP:journals/siamcomp/McCreight85}, the External Memory Priority Search Tree by Arge et al~\cite{arge_samoladas_vitter_1999}, and the External Memory Buffered Priority Search Tree by Brodal~\cite{DBLP:journals/corr/Brodal15}. We have also implemented wrappers around MySQL 5.7.12, Boost R-Tree, and libspatial external memory R*-Tree, such that the implementation match the interface of a Priority Search Tree. All the above structures have been thoroughly described and analysed in the I/O model, and for the internal memory structures we have argued about the complexity in the RAM model and concluded that the complexity can be adopted directly in the I/O model due to the obliviousness and inefficiency of access to data not available in memory.

We have conducted several experiments to compare the structures on the three important operations in the Priority Search Tree interface: \texttt{insert}, \texttt{delete}, and \texttt{query}.

Our results show that the External Memory Buffered Priority Search Tree outperforms all other structures except for MySQL without an index when it comes to inserting uniformly random data. Of the other structures it is worth mentioning that the Boost R-Tree performed best due to its efficient use of space.

When we looked at deleting uniformly random data, MySQL without an index expectedly fell short, and as soon as the structures not optimized for external memory had to operate outside of main memory we saw that the External Memory Buffered Priority Search Tree outperformed all other structures significantly.

We experimented with querying in two different ways to encapsulate the fact that the asymptotic complexity depends on both search and reporting. We isolated the search part by fixing 5 query windows of different height with 5Mb data each and then insert uniformly random data around to add noise. The results shows that the internal memory once again perform very well while still contained in main memory, but as soon as we leave main memory the only real contestant to the External Memory Buffered Priority Search is MySQL with an index which is once again outperformed for large enough input size.

\todo{conclude on focus on reporting part of experiments}

Our experiments also showed that our implementation of the External Memory Priority Search Tree proved inferior in all aspects which we explain by the large number of I/O and heavy internal work.

\section{Future work}
The structure of Brodal described in Chapter~\ref{chp:epst} is also able to report top-$k$ queries. A top-$k$ query reports the top $k$ points in the query range. The structure does this in $\mathcal{O}(\frac{1}{\epsilon}\log_B N + K/B)$, i.e. in the same bound as three-sided range queries, but here $K$ is the minimum of $k$ and the number of points in the query range. It would have been interesting to see how the structure performs these top-$k$ queries compared to other results.

Many of the structures discussed in this thesis also describes a construction algorithm. In future work it could interesting to see how the structures compare in terms of construction time.

\todo{replace std::set}

\todo{non uniform worst case reporting experiment}

\clearpage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{references}

\clearpage

\appendix

\chapter{Indexing for Data Models with Constraints and Classes}
\label{chp:kanellakis}

Kanellakis et al. presents a linear space and partially dynamic data structure in~\cite{Kanellakis1996589}. The data structure answers three-sided queries in $\mathcal{O}(\log_B N + \nicefrac{K}{B} + \log_2 B)$ I/O's and supports inserts in $\mathcal{O}(\log_B N + (\log^2_B \nicefrac{N}{B})$ I/O's. The result is fairly involved and is unlikely to perform well in any practical manner.

We will omit the full details and only present the overall ideas. As a first step all points are shifted such that they lie above the line $y = x$. The basic building block of the data structure is the \textit{metablock tree}; a B-ary tree of \textit{metablocks}, each of which represents $B^2$ data points. The root represents the $B^2$ data points with the largest $y$-values. The remaining $N - B^2$ data points are divided into $B$ groups of $(N - B^2)/B$ data points each based on the $x$-coordinate. The first group contains the $(N - B^2)/B$ data points with the smallest $x$-values and so on. A recursive tree of the exact same type is constructed for each such group of data points. This process continues until a group has at most $B^2$ data points and can fit into a single metablock. Refer to Figure~\ref{fig:kanellakis_metablock_tree} for an illustration of a metablock tree.

\begin{figure}[H]
	\centering
		\includegraphics[width=0.40\textwidth]{../figures/kanellakis_metablock_tree}
	\caption{A metablock tree for $B=3$ and $N=70$. All data points lie above the line $y=x$. Each region represents a metablock. The root is at the top. Note that each nonleaf metablock contains $B^2 = 9$ data points.}
	\label{fig:kanellakis_metablock_tree}
\end{figure}

Points from each $metablock$ are copied into new blocks that are both vertically and horizontally oriented as depicted in Figure~\ref{fig:kanellakis_vertical_horizontally_oriented_blocks}. 

\begin{figure}[h]
	\centering
		\includegraphics[width=0.5\textwidth]{../figures/kanellakis_vertical_horizontally_oriented_blocks}
	\caption{Vertically and horizontally oriented blockings of data points. Each rectangle represents a block: (a) vertically oriented; (b) horizontally oriented.}
	\label{fig:kanellakis_vertical_horizontally_oriented_blocks}
\end{figure}

Finally, each metablock $M$ contains pointers to B blocks that represents the set $TS(M)$ that is obtained by examining the left siblings of $M$ and taking the $B^2$ largest points according to the $y$-value. Depending on how the query spans the metablock tree we can query the auxiliary data structures in such a way that we can achieve the promised $\mathcal{O}(\log_B N + \nicefrac{K}{B} + log_2 B)$ I/O's. The five different query cases are depicted in Figure~\ref{fig:kanellakis_queries}.

\begin{figure}[h]
	\centering
		\includegraphics[width=1\textwidth]{../figures/kanellakis_queries}
	\caption{The three-sided queries can span the metablock tree in five different ways.}
	\label{fig:kanellakis_queries}
\end{figure}

\chapter{Path Caching}
\label{chp:ramaswamy}

Ramaswamy and Subramanian presents a suboptimal space data structure that answers three-sided queries with an optimal query bound in \cite{Ramaswamy:1994:PCT:182591.182595}. They use the same basic blocked B-Tree with pointers to full buckets of data points as introduced by Icking et al.~\cite{Icking1988} and illustrated in Figure~\ref{fig:icking_external_pst}. In addition they introduce the idea of \textit{path caching} that we will explain shortly. It can be seen that by using B-Tree we are able to answer two-sided queries in $\mathcal{O}(\log N + \nicefrac{K}{B})$ I/O's by classifying points inside the query into four categories as follows:

\begin{itemize}
	\item \textit{Corner}: this is the node whose region contains the corner of the query.
	\item \textit{Ancestors of the corner}: These are nodes whose regions are cut by the left side of the query. There can be at most $\mathcal{O}(\log N)$ such nodes.
	\item \textit{Right siblings of the corner and the ancestors}: these are nodes whose parents' regions are cut by the left side of the query. There can be at most $ \mathcal{O}(\log N)$ such nodes.
	\item \textit{Descendants of right siblings}: there can be an unbounded number of them, but for every such node, its parent's region has to be completely contained inside the query. That pays for the cost of looking into these nodes. That is, for every $J$ descendant blocks that are partially cut by the query, there will be at least $\nicefrac{J}{2}$ blocks that lie completely inside the query. 

\end{itemize} 

Please refer to Figure~\ref{fig:ramaswamy_query} for an illustration of the categories.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.9\textwidth]{../figures/ramaswamy_query}
	\caption{Binary tree implementation of Priority Search Tree in secondary memory showing a corner, ancestor sibling and sibling's descendant. Here, B is 4.}
	\label{fig:ramaswamy_query}
\end{figure}

Querying is done by locating the nodes intersecting the left side of the query using the B-Tree. Points from the nodes are reported by examining the associated buckets. Next, right siblings of the nodes and their descendants are examined in a top-down fashion until the bottom boundary of the query is crossed. It is crucial to note that the corner, ancestor, and sibling nodes can cause wasteful I/O's. Thus there are $\mathcal{O}(\log N)$ wasteful nodes as every parent of a visited node would have contributed an useful I/O. From this analysis we can conclude that we can answer two-sided queries in $\mathcal{O}(\log N + \nicefrac{K}{B})$ I/O's. Now, we are able to avoid the wasteful I/O's by caching the data in the ancestor and sibling nodes. By associating two caches with the corner that contain all data in the siblings sorted by $x$-coordinate and $y$-coordinate respectively, we are able to answer queries in $\mathcal{O}\left(\log_B N + \nicefrac{K}{B}\right)$ I/O's by simply locating the corner in $\mathcal{O}(\log_B N)$ I/O's and report using the cache. The storage usage is $\mathcal{O}(\nicefrac{N}{B} \log N)$ disk blocks of size $B$ each. The idea can be extended to handle three-sided queries by adding additional caches that cover point sets sorted from right to left. This gives a space usage of $\mathcal{O}(\nicefrac{N}{B} \log^2 N)$. 

\section{Better space bounds}
\label{sec:ramaswamy_better_space}

Ramaswamy and Subramanian brings down the space usage in~\cite{Subramanian:1995:PTN:313651.313769}. This is done by building a search tree that divides the points into smaller regions of size $B \log B$ instead of $B$ giving a total of $\nicefrac{N}{B} \log B$ regions. Now a slightly modified caching scheme is used with an additional secondary level structure for each region giving a total space usage of $\mathcal{O}(\nicefrac{N}{B} \log \log B)$. Reusing this idea in a multilevel scheme brings down the data structure to a $\mathcal{O}(\nicefrac{N}{B} \log B \log^* B)$ space solution that answers queries in $\mathcal{O}(\log_B + \nicefrac{K}{B} + \mathcal{IL}^*(B))$, where $\mathcal{IL^*}(x)$ denotes the number of times $\log^*$ must be applied before the result becomes $\leq 2$.


\chapter{Data}
\label{app:data}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{../src/experiments/gerth_buffer_size_experiment_results/2016-05-03.13_51_54/pfs}
\caption{The measured number of page faults when inserting 5Gb of data in the External Memory Buffered Priority Search Tree. We see that a buffer size of 32Mb will generate page faults.}
\label{fig:page_faults_for_large_buffer_size}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{../src/experiments/delete_experiment_results/2016-05-31.08_55_51/pfs}
\caption{Experimentally measured number of page faults when deleting points in the Boost R-Tree from a size of 400Mb down to a size of 350Mb.}
\label{fig:page_faults_boost_r_tree}
\end{figure}

\end{document}
